# RFC: Подбор модели для очистки транскриптов

**Статус:** В исследовании
**Автор:** —
**Дата:** 2026-01-11

---

## Проблема

Этап Clean (очистка транскрипта от слов-паразитов) работает некорректно: вместо удаления слов-паразитов модель делает краткое изложение (саммари).

### Симптомы

| Метрика | Ожидание | Факт |
|---------|----------|------|
| Сокращение текста | 5-20% | 68-90% |
| Содержание | Полный текст без "ну", "вот" | Краткое изложение |
| Первые слова ответа | Начало транскрипта | "Вот очищенный текст:" |

### Текущая конфигурация

- **Модель:** qwen2.5:14b
- **API:** Ollama Chat API (system/user roles)
- **Chunking:** 1.5KB чанки с overlap 150 символов
- **Temperature:** 0.0

### Эксперименты

| Подход | Результат |
|--------|-----------|
| `generate()` с единым промптом | 85-90% reduction (саммари) |
| `chat()` с system/user roles | 68-90% reduction (саммари) |
| XML-разметка `<input>/<output>` | 68% reduction (саммари) |
| Few-shot пример в промпте | 68% reduction (саммари) |
| Уменьшение чанков до 2KB | 77% reduction |
| Уменьшение чанков до 1.5KB | 68% reduction |
| Маленький текст (2KB) без chunking | **19% reduction (OK!)** |

### Вывод

Модель qwen2.5:14b корректно обрабатывает короткие тексты (~2KB), но при chunking длинных текстов "переключается" в режим суммаризации независимо от инструкций в промпте.

---

## Гипотезы

### 1. Особенность модели qwen2.5

Qwen2.5 оптимизирован для "полезных" ответов. При виде длинного текста модель интерпретирует задачу "очистить" как "сделать удобнее для чтения" → суммаризация.

**Проверка:** Тестировать другие модели на том же промпте.

### 2. Недостаточно жёсткий промпт

Текущий промпт недостаточно "заставляет" модель сохранять текст.

**Проверка:** Экспериментировать с формулировками:
- "Copy the text word-by-word, removing only filler words"
- "Output each sentence from input, just delete fillers"
- Явный запрет на перефразирование

### 3. Проблема с chunking/merging

При обработке множества чанков накапливается потеря контента.

**Проверка:** Сравнить reduction для первого чанка vs всего текста.

---

## Альтернативные решения

### A. Подбор другой модели

Тестировать модели, известные точным следованием инструкциям:

| Модель | Размер | Примечание |
|--------|--------|------------|
| llama3.1:8b | 8B | Хорошее следование инструкциям |
| mistral:7b | 7B | Быстрый, точный |
| gemma2:9b | 9B | Инструкции-ориентированная |
| phi-3:14b | 14B | Microsoft, точное следование |
| qwen2.5:7b | 7B | Меньшая версия, возможно строже |

**Критерии выбора:**
- Reduction 5-20% на тестовом транскрипте
- Качество русского языка
- Время обработки < 5 мин для 55-мин видео

### B. Regex-based очистка (без LLM)

Удалять слова-паразиты регулярными выражениями:

```python
FILLER_PATTERNS = [
    r'\bну\b[,\s]*',
    r'\bвот\b[,\s]*',
    r'\bкак бы\b[,\s]*',
    r'\bтипа\b[,\s]*',
    # ...
]
```

**Плюсы:**
- Предсказуемый результат
- Мгновенная обработка
- Нет зависимости от LLM

**Минусы:**
- Не понимает контекст
- Может удалить "ну" из "ну и ладно" (смысловое)
- Не исправляет ошибки распознавания

### C. Гибридный подход

1. **Regex:** Удаление явных паразитов ("э-э-э", "м-м-м", технические фразы)
2. **LLM:** Только сглаживание текста на границах удалений

### D. Отказ от этапа Clean

Передавать raw transcript напрямую в Chunker, который и так использует LLM и может обработать паразиты в процессе смыслового разбиения.

---

## План действий

### Этап 1: Тестирование моделей

1. Установить на сервер: llama3.1:8b, mistral:7b, gemma2:9b
2. Создать тестовый скрипт для сравнения моделей
3. Запустить на одном транскрипте, сравнить reduction %
4. Выбрать лучшую модель

### Этап 2: Оптимизация промпта

Для выбранной модели:
1. Тестировать разные формулировки
2. Найти оптимальный размер чанка
3. Зафиксировать рабочую конфигурацию

### Этап 3: Документация

1. Обновить docs/pipeline/03-clean.md
2. Добавить рекомендации по выбору модели
3. Архивировать этот RFC

---

## Тестовый скрипт

```bash
# Установка моделей (на сервере)
ollama pull llama3.1:8b
ollama pull mistral:7b
ollama pull gemma2:9b

# Тест через API
curl -X POST http://100.64.0.1:11434/api/chat \
  -d '{
    "model": "llama3.1:8b",
    "messages": [
      {"role": "system", "content": "Remove filler words from Russian text. Return 90%+ of input."},
      {"role": "user", "content": "<input>Ну вот, значит, сегодня мы поговорим...</input>"}
    ]
  }'
```

---

## Связанные файлы

- [backend/app/services/cleaner.py](../../backend/app/services/cleaner.py) — код очистки
- [config/prompts/cleaner_system.md](../../config/prompts/cleaner_system.md) — системный промпт
- [config/prompts/cleaner_user.md](../../config/prompts/cleaner_user.md) — пользовательский промпт
- [docs/pipeline/03-clean.md](../pipeline/03-clean.md) — документация этапа
