# Pipeline –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ

> –î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —ç—Ç–∞–ø–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ –æ—Ç inbox –¥–æ –≥–æ—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –ë–ó 2.0.

## –û–±–∑–æ—Ä Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      VIDEO PROCESSING PIPELINE                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ 1.PARSE ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ2.WHISPER‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ3.CLEAN  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ4.CHUNK  ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ filename‚îÇ    ‚îÇtranscr. ‚îÇ    ‚îÇ + gloss ‚îÇ    ‚îÇsemantic ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                                                   ‚îÇ             ‚îÇ
‚îÇ                                                   ‚ñº             ‚îÇ
‚îÇ                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ                              ‚îÇ6.SAVE   ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ5.SUMMAR.‚îÇ         ‚îÇ
‚îÇ                              ‚îÇ files   ‚îÇ    ‚îÇ + class ‚îÇ         ‚îÇ
‚îÇ                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —ç—Ç–∞–ø–æ–≤

| –≠—Ç–∞–ø | –ù–∞–∑–≤–∞–Ω–∏–µ | –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç | –í—Ö–æ–¥ | –í—ã—Ö–æ–¥ |
|------|----------|------------|------|-------|
| 1 | Parse Filename | Python regex | `*.mp4` filename | `VideoMetadata` |
| 2 | Transcribe | faster-whisper | `*.mp4` file | `RawTranscript` |
| 3 | Clean | Ollama + Glossary | `RawTranscript` | `CleanedTranscript` |
| 4 | Chunk | Ollama | `CleanedTranscript` | `TranscriptChunks` |
| 5 | Summarize | Ollama | `CleanedTranscript` | `Summary` + classification |
| 6 | Save | Python | All data | Files in archive |

---

## –≠—Ç–∞–ø 1: Parse Filename

### –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ

–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –ø–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–º—É –ø–∞—Ç—Ç–µ—Ä–Ω—É.

### –ü–∞—Ç—Ç–µ—Ä–Ω –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞

```
{–¥–∞—Ç–∞} {—Ç–∏–ø}.{–ø–æ—Ç–æ–∫} {—Ç–µ–º–∞} ({—Å–ø–∏–∫–µ—Ä}).mp4

–ü—Ä–∏–º–µ—Ä:
2025.04.07 –ü–®.SV –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –ì—Ä—É–ø–ø—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏ (–°–≤–µ—Ç–ª–∞–Ω–∞ –î–º–∏—Ç—Ä—É–∫).mp4
```

### Regex

```python
FILENAME_PATTERN = r'^(\d{4}\.\d{2}\.\d{2})\s+(\w+)\.(\w+)\s+(.+?)\s+\(([^)]+)\)(?:\.\w+)?$'

# –ì—Ä—É–ø–ø—ã:
# 1: date       (2025.04.07)
# 2: event_type (–ü–®)
# 3: stream     (SV)
# 4: title      (–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –ì—Ä—É–ø–ø—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏)
# 5: speaker    (–°–≤–µ—Ç–ª–∞–Ω–∞ –î–º–∏—Ç—Ä—É–∫)
```

### –ú–æ–¥–µ–ª—å –¥–∞–Ω–Ω—ã—Ö

```python
from dataclasses import dataclass
from datetime import date
from pathlib import Path

@dataclass
class VideoMetadata:
    """–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤–∏–¥–µ–æ, –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞."""
    
    # –ò–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    date: date                    # 2025-04-07
    event_type: str               # –ü–®
    stream: str                   # SV
    title: str                    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –ì—Ä—É–ø–ø—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏
    speaker: str                  # –°–≤–µ—Ç–ª–∞–Ω–∞ –î–º–∏—Ç—Ä—É–∫
    
    # –í—ã—á–∏—Å–ª—è–µ–º—ã–µ
    original_filename: str        # –ü–æ–ª–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
    video_id: str                 # 2025-04-07_psh-sv_gruppa-podderzhki
    
    # –ü—É—Ç–∏
    source_path: Path             # /inbox/filename.mp4
    archive_path: Path            # /archive/2025/04/–ü–®.SV/Title (Speaker)/
    
    @property
    def stream_full(self) -> str:
        """–ü–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ—Ç–æ–∫–∞ –∏–∑ config/events.yaml."""
        # –ü–®.SV -> –ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏—á–Ω–∞—è –®–∫–æ–ª–∞ ‚Äî –°—É–ø–µ—Ä–≤–∞–π–∑–µ—Ä—ã
        pass
```

### –ì–µ–Ω–µ—Ä–∞—Ü–∏—è video_id

```python
def generate_video_id(metadata: VideoMetadata) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –≤–∏–¥–µ–æ.
    
    –§–æ—Ä–º–∞—Ç: {date}_{event_type}-{stream}_{slug}
    –ü—Ä–∏–º–µ—Ä: 2025-04-07_psh-sv_gruppa-podderzhki
    """
    date_str = metadata.date.isoformat()  # 2025-04-07
    event_stream = f"{metadata.event_type}-{metadata.stream}".lower()  # psh-sv
    slug = slugify(metadata.title)  # gruppa-podderzhki
    
    return f"{date_str}_{event_stream}_{slug}"
```

### Error Handling

```python
class FilenameParseError(Exception):
    """–ò–º—è —Ñ–∞–π–ª–∞ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—É."""
    pass

def parse_filename(filename: str) -> VideoMetadata:
    match = re.match(FILENAME_PATTERN, filename)
    if not match:
        raise FilenameParseError(
            f"–§–∞–π–ª '{filename}' –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—É. "
            f"–û–∂–∏–¥–∞–µ—Ç—Å—è: '{{–¥–∞—Ç–∞}} {{—Ç–∏–ø}}.{{–ø–æ—Ç–æ–∫}} {{—Ç–µ–º–∞}} ({{—Å–ø–∏–∫–µ—Ä}}).mp4'"
        )
    # ...
```

---

## –≠—Ç–∞–ø 2: Transcribe (Whisper)

### –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ

–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∞—É–¥–∏–æ –≤ —Ç–µ–∫—Å—Ç —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤.

### –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç

**faster-whisper-server** ‚Äî REST API —Å–µ—Ä–≤–µ—Ä –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏, —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π –Ω–∞ TrueNAS.

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –ó–Ω–∞—á–µ–Ω–∏–µ |
|----------|----------|
| API URL | http://100.64.0.1:9000 |
| –ú–æ–¥–µ–ª—å | large-v3 (–ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–∞) |
| GPU | RTX 5070 Ti |

### –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

```python
import requests

WHISPER_CONFIG = {
    "api_url": "http://100.64.0.1:9000",
    "language": "ru",              # –†—É—Å—Å–∫–∏–π —è–∑—ã–∫
    "response_format": "verbose_json",  # JSON —Å —Ç–∞–π–º–∫–æ–¥–∞–º–∏
    "timeout": 600,                # 10 –º–∏–Ω—É—Ç –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ
}
````

### –ú–æ–¥–µ–ª—å –¥–∞–Ω–Ω—ã—Ö

```python
@dataclass
class TranscriptSegment:
    """–û–¥–∏–Ω —Å–µ–≥–º–µ–Ω—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –æ—Ç Whisper."""
    
    start: float          # –ù–∞—á–∞–ª–æ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (15.5)
    end: float            # –ö–æ–Ω–µ—Ü –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (18.2)
    text: str             # –¢–µ–∫—Å—Ç —Å–µ–≥–º–µ–Ω—Ç–∞
    
    @property
    def start_time(self) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ (00:00:15)."""
        return self._format_time(self.start)
    
    @property
    def end_time(self) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è –∫–æ–Ω—Ü–∞ (00:00:18)."""
        return self._format_time(self.end)
    
    @staticmethod
    def _format_time(seconds: float) -> str:
        h = int(seconds // 3600)
        m = int((seconds % 3600) // 60)
        s = int(seconds % 60)
        return f"{h:02d}:{m:02d}:{s:02d}"


@dataclass
class RawTranscript:
    """–°—ã—Ä–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –æ—Ç Whisper."""
    
    segments: list[TranscriptSegment]
    language: str                    # –û–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π —è–∑—ã–∫
    duration_seconds: float          # –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤–∏–¥–µ–æ
    whisper_model: str               # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
    
    @property
    def full_text(self) -> str:
        """–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ —Ç–∞–π–º-–∫–æ–¥–æ–≤."""
        return " ".join(seg.text for seg in self.segments)
    
    @property
    def text_with_timestamps(self) -> str:
        """–¢–µ–∫—Å—Ç —Å —Ç–∞–π–º-–∫–æ–¥–∞–º–∏ –¥–ª—è LLM –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
        lines = []
        for seg in self.segments:
            lines.append(f"[{seg.start_time}] {seg.text}")
        return "\n".join(lines)
```

### –ü—Ä–æ—Ü–µ—Å—Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏

```python
import requests
from pathlib import Path

async def transcribe(video_path: Path, config: dict) -> RawTranscript:
    """
    –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ Whisper HTTP API.
    
    1. –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ñ–∞–π–ª –Ω–∞ —Å–µ—Ä–≤–µ—Ä faster-whisper
    2. –ü–æ–ª—É—á–∞–µ—Ç JSON —Å —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    3. –°–æ–±–∏—Ä–∞–µ—Ç —Å–µ–≥–º–µ–Ω—Ç—ã –≤ RawTranscript
    """
    
    url = f"{config['api_url']}/v1/audio/transcriptions"
    
    with open(video_path, "rb") as f:
        response = requests.post(
            url,
            files={"file": f},
            data={
                "language": config["language"],
                "response_format": config["response_format"],
            },
            timeout=config["timeout"]
        )
    
    response.raise_for_status()
    data = response.json()
    
    # –ü–∞—Ä—Å–∏–Ω–≥ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ –æ—Ç–≤–µ—Ç–∞ API
    transcript_segments = [
        TranscriptSegment(
            start=seg["start"],
            end=seg["end"],
            text=seg["text"].strip()
        )
        for seg in data.get("segments", [])
    ]
    
    return RawTranscript(
        segments=transcript_segments,
        language=data.get("language", config["language"]),
        duration_seconds=data.get("duration", 0),
        whisper_model="large-v3"
    )
```

### –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: –ø—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –±–µ–∑ —Å–µ–≥–º–µ–Ω—Ç–æ–≤

```python
async def transcribe_text_only(video_path: Path, config: dict) -> str:
    """
    –ë—ã—Å—Ç—Ä–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è ‚Äî —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç –±–µ–∑ —Ç–∞–π–º–∫–æ–¥–æ–≤.
    –ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–≥–¥–∞ —Å–µ–≥–º–µ–Ω—Ç—ã –Ω–µ –Ω—É–∂–Ω—ã.
    """
    
    url = f"{config['api_url']}/v1/audio/transcriptions"
    
    with open(video_path, "rb") as f:
        response = requests.post(
            url,
            files={"file": f},
            data={
                "language": config["language"],
                "response_format": "text",
            },
            timeout=config["timeout"]
        )
    
    response.raise_for_status()
    return response.text
```

### –ü—Ä–æ–≥—Ä–µ—Å—Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏

```python
# WebSocket updates –¥–ª—è UI
async def transcribe_with_progress(
    video_path: Path,
    config: dict,
    progress_callback: Callable[[float, str], None]
) -> RawTranscript:
    """
    –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è–º–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞.
    
    progress_callback(percent, status_message)
    
    –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: Whisper API –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏.
    –ü—Ä–æ–≥—Ä–µ—Å—Å –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –ø–æ —ç—Ç–∞–ø–∞–º: –æ—Ç–ø—Ä–∞–≤–∫–∞ ‚Üí –æ–±—Ä–∞–±–æ—Ç–∫–∞ ‚Üí –≥–æ—Ç–æ–≤–æ.
    """
    progress_callback(0, "–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∞–π–ª–∞...")
    
    # –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ —Ä–∞–∑–º–µ—Ä—É —Ñ–∞–π–ª–∞
    file_size_mb = video_path.stat().st_size / (1024 * 1024)
    estimated_seconds = file_size_mb * 0.5  # ~0.5 —Å–µ–∫ –Ω–∞ MB (—ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏)
    
    progress_callback(5, "–û—Ç–ø—Ä–∞–≤–∫–∞ –Ω–∞ —Å–µ—Ä–≤–µ—Ä Whisper...")
    
    url = f"{config['api_url']}/v1/audio/transcriptions"
    
    with open(video_path, "rb") as f:
        progress_callback(10, f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è (~{int(estimated_seconds)} —Å–µ–∫)...")
        
        response = requests.post(
            url,
            files={"file": f},
            data={
                "language": config["language"],
                "response_format": config["response_format"],
            },
            timeout=config["timeout"]
        )
    
    progress_callback(90, "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞...")
    
    response.raise_for_status()
    data = response.json()
    
    transcript_segments = [
        TranscriptSegment(
            start=seg["start"],
            end=seg["end"],
            text=seg["text"].strip()
        )
        for seg in data.get("segments", [])
    ]
    
    progress_callback(100, "–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    
    return RawTranscript(
        segments=transcript_segments,
        language=data.get("language", config["language"]),
        duration_seconds=data.get("duration", 0),
        whisper_model="large-v3"
    )
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–∞

```python
def check_whisper_available(config: dict) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ Whisper —Å–µ—Ä–≤–∏—Å –¥–æ—Å—Ç—É–ø–µ–Ω."""
    try:
        response = requests.get(
            f"{config['api_url']}/health",
            timeout=5
        )
        return response.text == "OK"
    except requests.RequestException:
        return False
```

### –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

|–ú–µ—Ç—Ä–∏–∫–∞|–ó–Ω–∞—á–µ–Ω–∏–µ|
|---|---|
|–ú–æ–¥–µ–ª—å|large-v3 (–ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ)|
|–ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ—Å–ª–µ –ø—Ä–æ—Å—Ç–æ—è|~65 —Å–µ–∫ (–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –≤ VRAM)|
|–ü–æ—Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–ø—Ä–æ—Å—ã|~4-5 —Å–µ–∫ –Ω–∞ 15 —Å–µ–∫ –∞—É–¥–∏–æ|
|VRAM|~3.5 GB|
|–¢–∞–π–º–∞—É—Ç –º–æ–¥–µ–ª–∏|5 –º–∏–Ω –Ω–µ–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏|

> **–í–∞–∂–Ω–æ:** –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ—Å–ª–µ –ø—Ä–æ—Å—Ç–æ—è —Å–µ—Ä–≤–µ—Ä–∞ –±—É–¥–µ—Ç –º–µ–¥–ª–µ–Ω–Ω—ã–º ‚Äî –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –≤ GPU –ø–∞–º—è—Ç—å. –ü–æ—Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–ø—Ä–æ—Å—ã –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –±—ã—Å—Ç—Ä–æ.

---

## –≠—Ç–∞–ø 3: Clean (LLM + Glossary)

### –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ

–û—á–∏—Å—Ç–∫–∞ —Å—ã—Ä–æ–≥–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ –æ—Ç —à—É–º–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏.

### –ü—Ä–æ–±–ª–µ–º—ã —Å—ã—Ä–æ–≥–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞

| –ü—Ä–æ–±–ª–µ–º–∞ | –ü—Ä–∏–º–µ—Ä | –†–µ—à–µ–Ω–∏–µ |
|----------|--------|---------|
| –°–ª–æ–≤–∞-–ø–∞—Ä–∞–∑–∏—Ç—ã | "–Ω—É", "–≤–æ—Ç", "–∫–∞–∫ –±—ã", "—ç—ç—ç" | LLM —É–¥–∞–ª—è–µ—Ç |
| –û—Ç–≤–ª–µ—á–µ–Ω–∏—è | "–∫—Å—Ç–∞—Ç–∏, –≤—á–µ—Ä–∞ —è..." | LLM —É–¥–∞–ª—è–µ—Ç |
| –û—à–∏–±–∫–∏ Whisper | "–§–æ—Ä–º—É–ª–∞ –æ–¥–∏–Ω" | –ì–ª–æ—Å—Å–∞—Ä–∏–π –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç |
| –¢–µ—Ä–º–∏–Ω—ã Herbalife | "–≥–µ—Ä–±–∞–ª–∞–π—Ñ" | –ì–ª–æ—Å—Å–∞—Ä–∏–π –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç |

### –î–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞

```
RawTranscript
     ‚îÇ
     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3a. GLOSSARY    ‚îÇ  –ë—ã—Å—Ç—Ä–∞—è –∑–∞–º–µ–Ω–∞ –ø–æ —Å–ª–æ–≤–∞—Ä—é
‚îÇ    (Python)     ‚îÇ  
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3b. LLM CLEAN   ‚îÇ  –£–¥–∞–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–∑–∏—Ç–æ–≤ –∏ –æ—Ç–≤–ª–µ—á–µ–Ω–∏–π
‚îÇ    (Ollama)     ‚îÇ  
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
  CleanedTranscript
```

### 3a. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≥–ª–æ—Å—Å–∞—Ä–∏—è

–ú–µ—Ç–æ–¥ `_apply_glossary()` –∫–ª–∞—Å—Å–∞ `TranscriptCleaner`:

```python
def _apply_glossary(self, text: str) -> tuple[str, list[str]]:
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç –∑–∞–º–µ–Ω—ã —Ç–µ—Ä–º–∏–Ω–æ–≤ –ø–æ –≥–ª–æ—Å—Å–∞—Ä–∏—é.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        tuple: (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, —Å–ø–∏—Å–æ–∫ –∑–∞–º–µ–Ω)
    """
    corrections = []
    replacements = []

    # –°–æ–±–∏—Ä–∞–µ–º –∑–∞–º–µ–Ω—ã –∏–∑ –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–ª—é—á–∏ (version, date, total_terms)
    for category_name, terms in self.glossary.items():
        if not isinstance(terms, list):
            continue

        for term in terms:
            canonical = term.get("canonical")
            variations = term.get("variations", [])

            if not canonical or not variations:
                continue

            for variation in variations:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –≤–∞—Ä–∏–∞—Ü–∏—è —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–∞–Ω–æ–Ω–∏–∫–æ–º
                if variation.lower() == canonical.lower():
                    continue
                replacements.append((variation, canonical))

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ (–¥–ª–∏–Ω–Ω—ã–µ –ø–µ—Ä–≤—ã–º–∏)
    replacements.sort(key=lambda x: len(x[0]), reverse=True)

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∑–∞–º–µ–Ω—ã
    for variation, canonical in replacements:
        pattern = rf"\b{re.escape(variation)}\b"
        matches = re.findall(pattern, text, flags=re.IGNORECASE)
        if matches:
            text = re.sub(pattern, canonical, text, flags=re.IGNORECASE)
            for match in matches:
                corrections.append(f"{match} -> {canonical}")

    return text, corrections
```

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–º–µ–Ω (–Ω–∞–ø—Ä. `["–≥–µ—Ä–±–∞–ª–∞–π—Ñ -> Herbalife", "–°–í -> –°—É–ø–µ—Ä–≤–∞–π–∑–µ—Ä"]`)
- –ü—Ä–æ–ø—É—Å–∫–∞–µ—Ç —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–ª—é—á–∏ –≥–ª–æ—Å—Å–∞—Ä–∏—è (`version`, `date`, `total_terms`)
- –ü—Ä–æ–ø—É—Å–∫–∞–µ—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏, —Å–æ–≤–ø–∞–¥–∞—é—â–∏–µ —Å –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–º –Ω–∞–ø–∏—Å–∞–Ω–∏–µ–º
- –†–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–π –ø–æ–∏—Å–∫ —Å –≥—Ä–∞–Ω–∏—Ü–∞–º–∏ —Å–ª–æ–≤ (`\b`)

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ glossary.yaml

–ì–ª–æ—Å—Å–∞—Ä–∏–π —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–æ–ª—è–º–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:

```yaml
# config/glossary.yaml

version: "3.0"
date: "2025-01-08"
total_terms: 79

# –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–µ—Ä–º–∏–Ω–æ–≤
statuses:
  - canonical: "–°—É–ø–µ—Ä–≤–∞–π–∑–µ—Ä"
    english: "Supervisor"
    description: "–ü–µ—Ä–≤—ã–π –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å (2500 VP + –≥—Ä—É–ø–ø–∞)"
    variations:
      - "—Å—É–ø–µ—Ä–≤–∞–π–∑–µ—Ä"
      - "—Å—É–ø–µ—Ä–≤–∏–∑–µ—Ä"
      - "—Å—É–ø–µ—Ä–≤–∞–π–∑–æ—Ä"
      - "–°–í"
      - "super visor"
      - "SV"
    context: "–≤—ã—à–µ–ª –Ω–∞ —Å—É–ø–µ—Ä–≤–∞–π–∑–µ—Ä–∞"

  - canonical: "–ì–ï–¢"
    english: "GET"
    description: "–ö–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π —Å—Ç–∞—Ç—É—Å (10,000 Group VP + 3 –ª–∏–Ω–∏–∏ –ø–æ 2500+ VP)"
    variations:
      - "–≥–µ—Ç"
      - "–≥–µ—Ç —Ç–∏–º"
      - "–≥–µ—Ç-—Ç–∏–º"
      - "Get Team"
      - "GET"
    context: "—è —É–∂–µ 5 –ª–µ—Ç –ì–ï–¢"

products:
  - canonical: "–§–æ—Ä–º—É–ª–∞ 1"
    english: "Formula 1"
    description: "–ü—Ä–æ—Ç–µ–∏–Ω–æ–≤—ã–π –∫–æ–∫—Ç–µ–π–ª—å –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –≤–µ—Å–∞"
    variations:
      - "—Ñ–æ—Ä–º—É–ª–∞ 1"
      - "—Ñ–æ—Ä–º—É–ª–∞ –æ–¥–∏–Ω"
      - "–§1"
      - "F1"
      - "–∫–æ–∫—Ç–µ–π–ª—å"
      - "—à–µ–π–∫"

brand:
  - canonical: "Herbalife"
    english: "Herbalife"
    description: "–ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏"
    variations:
      - "–≥–µ—Ä–±–∞–ª–∞–π—Ñ"
      - "–≥–µ—Ä–±–æ –ª–∞–π—Ñ"
      - "—Ö–µ—Ä–±–∞–ª–∞–π—Ñ"
      - "herbal life"
      - "—Ö–µ—Ä–±–∞ –ª–∞–π—Ñ"
```

**–ü–æ–ª—è —Ç–µ—Ä–º–∏–Ω–∞:**
| –ü–æ–ª–µ | –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ | –û–ø–∏—Å–∞–Ω–∏–µ |
|------|--------------|----------|
| `canonical` | –î–∞ | –ö–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–µ –Ω–∞–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –∑–∞–º–µ–Ω—ã |
| `variations` | –î–∞ | –°–ø–∏—Å–æ–∫ –≤–∞—Ä–∏–∞—Ü–∏–π –¥–ª—è –ø–æ–∏—Å–∫–∞ |
| `english` | –ù–µ—Ç | –ê–Ω–≥–ª–∏–π—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ) |
| `description` | –ù–µ—Ç | –û–ø–∏—Å–∞–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–∞ (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ) |
| `context` | –ù–µ—Ç | –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ) |

> **–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –î–ª—è –∑–∞–º–µ–Ω—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ç–æ–ª—å–∫–æ `canonical` –∏ `variations`. –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ–ª—è ‚Äî —Å–ø—Ä–∞–≤–æ—á–Ω—ã–µ.

### 3b. LLM Clean (Ollama)

–ö–ª–∞—Å—Å `TranscriptCleaner` –∏—Å–ø–æ–ª—å–∑—É–µ—Ç `AIClient` –¥–ª—è –≤—ã–∑–æ–≤–∞ Ollama:

```python
class TranscriptCleaner:
    """–°–µ—Ä–≤–∏—Å –æ—á–∏—Å—Ç–∫–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤."""

    def __init__(self, ai_client: AIClient, settings: Settings):
        self.ai_client = ai_client
        self.settings = settings
        self.prompt_template = load_prompt("cleaner", settings)
        self.glossary = load_glossary(settings)

    async def clean(
        self,
        raw_transcript: RawTranscript,
        metadata: VideoMetadata,
    ) -> CleanedTranscript:
        """
        –û—á–∏—â–∞–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –≤ –¥–≤–∞ —ç—Ç–∞–ø–∞:
        1. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≥–ª–æ—Å—Å–∞—Ä–∏—è
        2. LLM –æ—á–∏—Å—Ç–∫–∞ —á–µ—Ä–µ–∑ Ollama
        """
        original_text = raw_transcript.full_text
        original_length = len(original_text)

        # –®–∞–≥ 1: –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≥–ª–æ—Å—Å–∞—Ä–∏—è
        text_after_glossary, corrections = self._apply_glossary(original_text)

        # –®–∞–≥ 2: LLM –æ—á–∏—Å—Ç–∫–∞
        prompt = self._build_prompt(text_after_glossary, metadata)
        cleaned_text = await self.ai_client.generate(prompt)
        cleaned_text = cleaned_text.strip()

        return CleanedTranscript(
            text=cleaned_text,
            original_length=original_length,
            cleaned_length=len(cleaned_text),
            corrections_made=corrections,
        )

    def _build_prompt(self, text: str, metadata: VideoMetadata) -> str:
        """–ü–æ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –≤ —à–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞."""
        return self.prompt_template.format(transcript=text)
```

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:**
```python
async with AIClient(settings) as client:
    cleaner = TranscriptCleaner(client, settings)
    cleaned = await cleaner.clean(raw_transcript, metadata)
    print(f"–û—á–∏—â–µ–Ω–æ: {cleaned.original_length} -> {cleaned.cleaned_length} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"–ó–∞–º–µ–Ω—ã: {cleaned.corrections_made}")
```

### –ü—Ä–æ–º–ø—Ç –æ—á–∏—Å—Ç–∫–∏ (config/prompts/cleaner.md)

–ü—Ä–æ–º–ø—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é `{transcript}`:

```markdown
–¢—ã ‚Äî —Ä–µ–¥–∞–∫—Ç–æ—Ä —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤ –æ–±—É—á–∞—é—â–∏—Ö –≤–∏–¥–µ–æ. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ—á–∏—Å—Ç–∏—Ç—å —Ç–µ–∫—Å—Ç –æ—Ç —Ä–µ—á–µ–≤–æ–≥–æ –º—É—Å–æ—Ä–∞, —Å–æ—Ö—Ä–∞–Ω–∏–≤ –≤–µ—Å—å —Å–º—ã—Å–ª–æ–≤–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç.

## –ß—Ç–æ –£–î–ê–õ–ò–¢–¨:

1. **–°–ª–æ–≤–∞-–ø–∞—Ä–∞–∑–∏—Ç—ã:** "–Ω—É", "–≤–æ—Ç", "–∫–∞–∫ –±—ã", "—Ç–∏–ø–∞", "–∫–æ—Ä–æ—á–µ", "–∑–Ω–∞—á–∏—Ç", "—Ç–∞–∫ —Å–∫–∞–∑–∞—Ç—å", "–≤ –æ–±—â–µ–º", "–Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ", "—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ", "–¥–æ–ø—É—Å—Ç–∏–º", "—Å–∫–∞–∂–µ–º —Ç–∞–∫"

2. **–ó–∞–ø–æ–ª–Ω–∏—Ç–µ–ª–∏ –ø–∞—É–∑:** "—ç-—ç-—ç", "–º-–º-–º", "–∞-–∞-–∞", –ø–æ–≤—Ç–æ—Ä—ã —Å–ª–æ–≤, –Ω–µ–∑–∞–∫–æ–Ω—á–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã

3. **–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ñ—Ä–∞–∑—ã:** "–≤—ã –º–µ–Ω—è —Å–ª—ã—à–∏—Ç–µ?", "–≤–∏–¥–Ω–æ —ç–∫—Ä–∞–Ω?", "—Å–µ–π—á–∞—Å –ø–æ–∫–∞–∂—É", "–ø–æ–¥–æ–∂–¥–∏—Ç–µ —Å–µ–∫—É–Ω–¥—É", "—Ç–∞–∫, –≥–¥–µ —ç—Ç–æ...", "–æ–π, –Ω–µ —Ç—É–¥–∞ –Ω–∞–∂–∞–ª"

4. **–û—Ç–≤–ª–µ—á–µ–Ω–∏—è:** –æ–±—Å—É–∂–¥–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º —Å–æ —Å–≤—è–∑—å—é, –ø–æ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–µ —Ä–∞–∑–≥–æ–≤–æ—Ä—ã, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –Ω–µ –ø–æ —Ç–µ–º–µ

## –ß—Ç–æ –°–û–•–†–ê–ù–ò–¢–¨:

1. **–í–µ—Å—å —Å–º—ã—Å–ª–æ–≤–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç** ‚Äî —Ñ–∞–∫—Ç—ã, —Å–æ–≤–µ—Ç—ã, –ø—Ä–∏–º–µ—Ä—ã, –∏—Å—Ç–æ—Ä–∏–∏
2. **–°—Ç—Ä—É–∫—Ç—É—Ä—É –∏–∑–ª–æ–∂–µ–Ω–∏—è** ‚Äî –ø–æ—Ä—è–¥–æ–∫ —Ç–µ–º, –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø–µ—Ä–µ—Ö–æ–¥—ã
3. **–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é** ‚Äî –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤, –±–∏–∑–Ω–µ—Å-—Ç–µ—Ä–º–∏–Ω—ã
4. **–ü—Ä—è–º—É—é —Ä–µ—á—å –∏ —Ü–∏—Ç–∞—Ç—ã** ‚Äî –µ—Å–ª–∏ —Å–ø–∏–∫–µ—Ä –∫–æ–≥–æ-—Ç–æ —Ü–∏—Ç–∏—Ä—É–µ—Ç
5. **–ß–∏—Å–ª–∞ –∏ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫—É** ‚Äî –¥–∞—Ç—ã, –ø—Ä–æ—Ü–µ–Ω—Ç—ã, –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞

## –ü—Ä–∞–≤–∏–ª–∞:

- –ù–ï –¥–æ–±–∞–≤–ª—è–π –Ω–∏—á–µ–≥–æ –æ—Ç —Å–µ–±—è
- –ù–ï –º–µ–Ω—è–π —Å–º—ã—Å–ª –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏–π
- –ù–ï —É–¥–∞–ª—è–π –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è, –µ—Å–ª–∏ –æ–Ω–∏ –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è ("–æ—á–µ–Ω—å-–æ—á–µ–Ω—å –≤–∞–∂–Ω–æ")
- –°–æ—Ö—Ä–∞–Ω—è–π —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–π —Å—Ç–∏–ª—å, –Ω–µ –¥–µ–ª–∞–π —Ç–µ–∫—Å—Ç "–∫–Ω–∏–∂–Ω—ã–º"
- –ò—Å–ø—Ä–∞–≤–ª—è–π –æ—á–µ–≤–∏–¥–Ω—ã–µ –æ—à–∏–±–∫–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏

## –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏ –ø–æ—è—Å–Ω–µ–Ω–∏–π.

---

–¢–†–ê–ù–°–ö–†–ò–ü–¢ –î–õ–Ø –û–ß–ò–°–¢–ö–ò:

{transcript}
```

> **–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤–∏–¥–µ–æ (`{title}`, `{speaker}`) –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å–∏–∏ –ø—Ä–æ–º–ø—Ç–∞.

### –ú–æ–¥–µ–ª—å –¥–∞–Ω–Ω—ã—Ö

```python
class CleanedTranscript(BaseModel):
    """–û—á–∏—â–µ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏."""

    text: str                              # –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    original_length: int                   # –î–ª–∏–Ω–∞ –¥–æ –æ—á–∏—Å—Ç–∫–∏ (—Å–∏–º–≤–æ–ª—ã)
    cleaned_length: int                    # –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏
    corrections_made: list[str] = []       # –°–ø–∏—Å–æ–∫ –∑–∞–º–µ–Ω –≥–ª–æ—Å—Å–∞—Ä–∏—è

# –ü—Ä–∏–º–µ—Ä corrections_made:
# [
#     "–≥–µ—Ä–±–∞–ª–∞–π—Ñ -> Herbalife",
#     "—Ñ–æ—Ä–º—É–ª–∞ –æ–¥–∏–Ω -> –§–æ—Ä–º—É–ª–∞ 1",
#     "–°–í -> –°—É–ø–µ—Ä–≤–∞–π–∑–µ—Ä"
# ]
```

**–ü—Ä–∏–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞:**
```python
CleanedTranscript(
    text="–°–µ–≥–æ–¥–Ω—è –º—ã –ø–æ–≥–æ–≤–æ—Ä–∏–º –æ Herbalife. –§–æ—Ä–º—É–ª–∞ 1 ‚Äî —ç—Ç–æ –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–¥—É–∫—Ç.",
    original_length=164,
    cleaned_length=90,
    corrections_made=["–≥–µ—Ä–±–∞–ª–∞–π—Ñ -> Herbalife", "—Ñ–æ—Ä–º—É–ª–∞ –æ–¥–∏–Ω -> –§–æ—Ä–º—É–ª–∞ 1"]
)
# –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ: 46%
```

---

## –≠—Ç–∞–ø 4: Chunk (Semantic Splitting)

### –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ

–†–∞–∑–±–∏–µ–Ω–∏–µ –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ –Ω–∞ —Å–º—ã—Å–ª–æ–≤—ã–µ –±–ª–æ–∫–∏ –¥–ª—è RAG-–ø–æ–∏—Å–∫–∞ –≤ –ë–ó 2.0.

### –¢–µ–∫—É—â–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è: –ü—Ä–æ—Å—Ç—ã–µ —á–∞–Ω–∫–∏ (–í–∞—Ä–∏–∞–Ω—Ç A)

**–í—ã–±—Ä–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥:** LLM —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–∞–º–æ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–µ –±–ª–æ–∫–∏ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –°–∞–º–æ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å —á–∞–Ω–∫–æ–≤ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç—Å—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –≤ –ø—Ä–æ–º–ø—Ç–µ.

| –ö—Ä–∏—Ç–µ—Ä–∏–π | –ó–Ω–∞—á–µ–Ω–∏–µ | –ü–æ—á–µ–º—É |
|----------|----------|--------|
| –†–∞–∑–º–µ—Ä chunk | 100-400 —Å–ª–æ–≤ (–æ–ø—Ç–∏–º—É–º 200-300) | –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è embeddings |
| –°–º—ã—Å–ª–æ–≤–∞—è –∑–∞–≤–µ—Ä—à—ë–Ω–Ω–æ—Å—Ç—å | –û–¥–Ω–∞ —Ç–µ–º–∞/–º—ã—Å–ª—å | Chunk –ø–æ–Ω—è—Ç–µ–Ω –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ |
| Overlap | –ù–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è | LLM –¥–µ–ª–∞–µ—Ç —á–∞–Ω–∫–∏ —Å–∞–º–æ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º–∏ |
| –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ | topic + text | –ú–∏–Ω–∏–º—É–º –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –∏ –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏ |

### –ö–ª–∞—Å—Å SemanticChunker

```python
class SemanticChunker:
    """–°–µ—Ä–≤–∏—Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤."""

    def __init__(self, ai_client: AIClient, settings: Settings):
        self.ai_client = ai_client
        self.settings = settings
        self.prompt_template = load_prompt("chunker", settings)

    async def chunk(
        self,
        cleaned_transcript: CleanedTranscript,
        metadata: VideoMetadata,
    ) -> TranscriptChunks:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –Ω–∞ —Å–º—ã—Å–ª–æ–≤—ã–µ —á–∞–Ω–∫–∏.

        Args:
            cleaned_transcript: –û—á–∏—â–µ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –æ—Ç cleaner
            metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤–∏–¥–µ–æ (–¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ID —á–∞–Ω–∫–æ–≤)

        Returns:
            TranscriptChunks —Å–æ —Å–ø–∏—Å–∫–æ–º —á–∞–Ω–∫–æ–≤
        """
        text = cleaned_transcript.text

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º replace() –≤–º–µ—Å—Ç–æ format() –∏–∑-–∑–∞ JSON –≤ –ø—Ä–æ–º–ø—Ç–µ
        prompt = self.prompt_template.replace("{transcript}", text)
        response = await self.ai_client.generate(prompt)

        # –ü–∞—Ä—Å–∏–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM
        chunks = self._parse_chunks(response, metadata.video_id)

        return TranscriptChunks(chunks=chunks)

    def _parse_chunks(self, response: str, video_id: str) -> list[TranscriptChunk]:
        """–ü–∞—Ä—Å–∏—Ç JSON –æ—Ç–≤–µ—Ç LLM –≤ —Å–ø–∏—Å–æ–∫ TranscriptChunk."""
        json_str = self._extract_json(response)
        data = json.loads(json_str)

        chunks = []
        for item in data:
            chunk = TranscriptChunk(
                id=f"{video_id}_{item['index']:03d}",
                index=item["index"],
                topic=item["topic"],
                text=item["text"],
                word_count=len(item["text"].split()),
            )
            chunks.append(chunk)
        return chunks

    def _extract_json(self, text: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM (–æ–±—Ä–∞–±–æ—Ç–∫–∞ markdown-–±–ª–æ–∫–æ–≤)."""
        # –£–¥–∞–ª—è–µ—Ç ```json ... ``` –æ–±—ë—Ä—Ç–∫—É –µ—Å–ª–∏ –µ—Å—Ç—å
        # –ù–∞—Ö–æ–¥–∏—Ç JSON –º–∞—Å—Å–∏–≤ [...] –≤ —Ç–µ–∫—Å—Ç–µ
        ...
```

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:**
```python
async with AIClient(settings) as client:
    chunker = SemanticChunker(client, settings)
    result = await chunker.chunk(cleaned_transcript, metadata)
    print(f"–°–æ–∑–¥–∞–Ω–æ {result.total_chunks} —á–∞–Ω–∫–æ–≤, avg {result.avg_chunk_size} —Å–ª–æ–≤")
```

### –ü—Ä–æ–º–ø—Ç chunking (config/prompts/chunker.md)

````markdown
–¢—ã ‚Äî —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—é —Ç–µ–∫—Å—Ç–∞. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Ä–∞–∑–±–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç
–Ω–∞ —Å–º—ã—Å–ª–æ–≤—ã–µ –±–ª–æ–∫–∏ (chunks) –¥–ª—è –ø–æ–∏—Å–∫–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã.

## –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –±–ª–æ–∫–∞–º:

1. **–†–∞–∑–º–µ—Ä:** 100-400 —Å–ª–æ–≤ (–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ 200-300)
2. **–°–º—ã—Å–ª–æ–≤–∞—è –∑–∞–≤–µ—Ä—à—ë–Ω–Ω–æ—Å—Ç—å:** –∫–∞–∂–¥—ã–π –±–ª–æ–∫ ‚Äî –æ–¥–Ω–∞ –∑–∞–∫–æ–Ω—á–µ–Ω–Ω–∞—è –º—ã—Å–ª—å –∏–ª–∏ —Ç–µ–º–∞
3. **–°–∞–º–æ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å:** –±–ª–æ–∫ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–Ω—è—Ç–µ–Ω –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥—Ä—É–≥–∏—Ö –±–ª–æ–∫–æ–≤
4. **–ë–µ–∑ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è:** –±–ª–æ–∫–∏ –Ω–µ –¥–æ–ª–∂–Ω—ã –ø–æ–≤—Ç–æ—Ä—è—Ç—å –¥—Ä—É–≥ –¥—Ä—É–≥–∞

## –ü—Ä–∞–≤–∏–ª–∞ —Ä–∞–∑–±–∏–µ–Ω–∏—è:

- –†–∞–∑–¥–µ–ª—è–π –ø–æ —Å–º–µ–Ω–µ —Ç–µ–º—ã –∏–ª–∏ –ø–æ–¥—Ç–µ–º—ã
- –ù–µ —Ä–∞–∑—Ä—ã–≤–∞–π –ø—Ä–∏–º–µ—Ä—ã –∏ –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ—Å–µ—Ä–µ–¥–∏–Ω–µ
- –°–æ—Ö—Ä–∞–Ω—è–π –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∫–∏ –≤ –Ω–∞—á–∞–ª–µ –±–ª–æ–∫–∞ ("–¢–∞–∫–∂–µ –≤–∞–∂–Ω–æ...", "–ï—â—ë –æ–¥–∏–Ω –º–æ–º–µ–Ω—Ç...")
- –ï—Å–ª–∏ —Ç–µ–º–∞ –±–æ–ª—å—à–∞—è ‚Äî —Ä–∞–∑–¥–µ–ª–∏ –Ω–∞ –ø–æ–¥—Ç–µ–º—ã

## –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:

–í–µ—Ä–Ω–∏ JSON-–º–∞—Å—Å–∏–≤ –æ–±—ä–µ–∫—Ç–æ–≤:

```json
[
  {"index": 1, "topic": "–ö—Ä–∞—Ç–∫–∞—è —Ç–µ–º–∞ –±–ª–æ–∫–∞", "text": "–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –±–ª–æ–∫–∞..."},
  {"index": 2, "topic": "–°–ª–µ–¥—É—é—â–∞—è —Ç–µ–º–∞", "text": "–¢–µ–∫—Å—Ç —Å–ª–µ–¥—É—é—â–µ–≥–æ –±–ª–æ–∫–∞..."}
]
```

–í–ê–ñ–ù–û: –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏ markdown-—Ä–∞–∑–º–µ—Ç–∫–∏.

---

–¢–†–ê–ù–°–ö–†–ò–ü–¢ –î–õ–Ø –†–ê–ó–ë–ò–ï–ù–ò–Ø:

{transcript}
````

### –ú–æ–¥–µ–ª—å –¥–∞–Ω–Ω—ã—Ö

```python
class TranscriptChunk(BaseModel):
    """–û–¥–∏–Ω —Å–º—ã—Å–ª–æ–≤–æ–π –±–ª–æ–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞."""

    id: str                # –§–æ—Ä–º–∞—Ç: {video_id}_{index:03d}
    index: int             # –ü–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä (1, 2, 3...)
    topic: str             # –ö—Ä–∞—Ç–∫–∞—è —Ç–µ–º–∞ –±–ª–æ–∫–∞ (3-7 —Å–ª–æ–≤)
    text: str              # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –±–ª–æ–∫–∞
    word_count: int        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ (–≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è)


class TranscriptChunks(BaseModel):
    """–†–µ–∑—É–ª—å—Ç–∞—Ç chunking."""

    chunks: list[TranscriptChunk]

    @computed_field
    def total_chunks(self) -> int:
        return len(self.chunks)

    @computed_field
    def avg_chunk_size(self) -> int:
        if not self.chunks:
            return 0
        return sum(c.word_count for c in self.chunks) // len(self.chunks)
```

---

### –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã (–¥–ª—è –±—É–¥—É—â–µ–≥–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è)

–¢–µ–∫—É—â–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –≤—ã–±—Ä–∞–Ω–∞ –∑–∞ –±–∞–ª–∞–Ω—Å –ø—Ä–æ—Å—Ç–æ—Ç—ã –∏ –∫–∞—á–µ—Å—Ç–≤–∞. –ù–∏–∂–µ ‚Äî –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –ø–æ—Å–ª–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

#### –í–∞—Ä–∏–∞–Ω—Ç B: –ß–∞–Ω–∫–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º

**–ò–¥–µ—è:** –î–æ–±–∞–≤–∏—Ç—å –ø–æ–ª–µ `context` —Å –∫—Ä–∞—Ç–∫–∏–º –æ–ø–∏—Å–∞–Ω–∏–µ–º –º–µ—Å—Ç–∞ —á–∞–Ω–∫–∞ –≤ –æ–±—â–µ–π —Ç–µ–º–µ –≤–∏–¥–µ–æ.

```python
class TranscriptChunk(BaseModel):
    id: str
    index: int
    topic: str
    text: str
    word_count: int
    context: str           # –ù–æ–≤–æ–µ: "–í —ç—Ç–æ–º –±–ª–æ–∫–µ —Å–ø–∏–∫–µ—Ä –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —Ç–µ–º—É –ø–∏—Ç–∞–Ω–∏—è..."
    related_topics: list[str]  # –ù–æ–≤–æ–µ: ["–º–∞–∫—Ä–æ–Ω—É—Ç—Ä–∏–µ–Ω—Ç—ã", "–±–µ–ª–∫–∏"]
```

| –ü–ª—é—Å—ã | –ú–∏–Ω—É—Å—ã |
|-------|--------|
| –õ—É—á—à–µ –¥–ª—è RAG-–ø–æ–∏—Å–∫–∞ | –°–ª–æ–∂–Ω–µ–µ –ø—Ä–æ–º–ø—Ç ‚Üí –±–æ–ª—å—à–µ –æ—à–∏–±–æ–∫ LLM |
| –ß–∞–Ω–∫ –ø–æ–Ω—è—Ç–µ–Ω –∞–≤—Ç–æ–Ω–æ–º–Ω–æ | –ë–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ ‚Üí –º–µ–¥–ª–µ–Ω–Ω–µ–µ |
| –°–≤—è–∑–∏ –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏ | –¢—Ä–µ–±—É–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã |

**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:** –ï—Å–ª–∏ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ RAG-–ø–æ–∏—Å–∫–∞ —á–∞–Ω–∫–∏ –æ–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º–∏ –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤—Å–µ–≥–æ –≤–∏–¥–µ–æ.

#### –í–∞—Ä–∏–∞–Ω—Ç C: –î–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞

**–ò–¥–µ—è:** –°–Ω–∞—á–∞–ª–∞ LLM —Å–æ–∑–¥–∞—ë—Ç "–ø–ª–∞–Ω" –≤—Å–µ–≥–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ (—Ç–µ–º—ã –∏ –∏—Ö –ø–æ—Ä—è–¥–æ–∫), –∑–∞—Ç–µ–º —Ä–∞–∑–±–∏–≤–∞–µ—Ç –Ω–∞ —á–∞–Ω–∫–∏, —Å—Å—ã–ª–∞—è—Å—å –Ω–∞ —ç—Ç–æ—Ç –ø–ª–∞–Ω.

```python
# –≠—Ç–∞–ø 1: –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑
structure = await ai_client.generate(structure_prompt)
# {"themes": ["–í–≤–µ–¥–µ–Ω–∏–µ", "–ü–∏—Ç–∞–Ω–∏–µ", "–í–∏—Ç–∞–º–∏–Ω—ã", "–ó–∞–∫–ª—é—á–µ–Ω–∏–µ"], ...}

# –≠—Ç–∞–ø 2: –ß–∞–Ω–∫–æ–≤–∞–Ω–∏–µ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
chunks = await ai_client.generate(chunk_prompt.format(
    transcript=text,
    structure=structure
))
```

| –ü–ª—é—Å—ã | –ú–∏–Ω—É—Å—ã |
|-------|--------|
| –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å | 2 –≤—ã–∑–æ–≤–∞ LLM –≤–º–µ—Å—Ç–æ 1 |
| –ü–æ–Ω–∏–º–∞–Ω–∏–µ –æ–±—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã | –°–∏–ª—å–Ω–æ —É—Å–ª–æ–∂–Ω—è–µ—Ç —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é |
| –õ—É—á—à–µ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ | –î–æ—Ä–æ–∂–µ –ø–æ —Ç–æ–∫–µ–Ω–∞–º |

**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:** –ï—Å–ª–∏ –≤–∏–¥–µ–æ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ (>1 —á–∞—Å) –∏ —Å–ø–∏–∫–µ—Ä —Å–∏–ª—å–Ω–æ "–ø—Ä—ã–≥–∞–µ—Ç" –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏, –≤–æ–∑–≤—Ä–∞—â–∞—è—Å—å –∫ –Ω–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑.

#### –í–∞—Ä–∏–∞–Ω—Ç D: –ì–∏–±—Ä–∏–¥–Ω—ã–π (LLM + —ç–≤—Ä–∏—Å—Ç–∏–∫–∏)

**–ò–¥–µ—è:** LLM –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≥—Ä–∞–Ω–∏—Ü—ã —Ç–µ–º, –Ω–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –¥–µ–ª–∞–µ—Ç—Å—è –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ –ø–æ —Ä–∞–∑–º–µ—Ä—É.

```python
# LLM –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –≥—Ä–∞–Ω–∏—Ü—ã
boundaries = await ai_client.generate(boundaries_prompt)
# [{"position": 0, "topic": "–í–≤–µ–¥–µ–Ω–∏–µ"}, {"position": 450, "topic": "–ü–∏—Ç–∞–Ω–∏–µ"}, ...]

# –ü—Ä–æ–≥—Ä–∞–º–º–Ω–æ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
chunks = split_by_boundaries_and_size(text, boundaries, max_words=400)
```

| –ü–ª—é—Å—ã | –ú–∏–Ω—É—Å—ã |
|-------|--------|
| –ö–æ–Ω—Ç—Ä–æ–ª—å —Ä–∞–∑–º–µ—Ä–∞ —á–∞–Ω–∫–æ–≤ | –°–ª–æ–∂–Ω–µ–µ –ª–æ–≥–∏–∫–∞ |
| –ú–µ–Ω—å—à–µ –Ω–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ LLM | –ú–æ–∂–µ—Ç —Ä–∞–∑—Ä–µ–∑–∞—Ç—å —Å–µ—Ä–µ–¥–∏–Ω—É –º—ã—Å–ª–∏ |
| –ü—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç | –¢–µ—Ä—è–µ–º "—É–º–Ω–æ–µ" —Ä–∞–∑–±–∏–µ–Ω–∏–µ LLM |

**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:** –ï—Å–ª–∏ LLM —Å–æ–∑–¥–∞—ë—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ –∏–ª–∏ –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∞–Ω–∫–∏, –∏ –Ω—É–∂–µ–Ω —Å—Ç—Ä–æ–≥–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—å —Ä–∞–∑–º–µ—Ä–∞ –¥–ª—è RAG.

---

## –≠—Ç–∞–ø 5: Summarize (+ Classification)

### –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ

–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–∞–º–º–∞—Ä–∏ –¥–ª—è File Search –≤ –ë–ó 2.0.

### LLM Summarization

```python
async def summarize_transcript(
    cleaned_text: str,
    metadata: VideoMetadata,
    client: AsyncClient
) -> dict:
    """
    –°–æ–∑–¥–∞—ë—Ç —Å–∞–º–º–∞—Ä–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –¥–ª—è –ë–ó 2.0.
    
    Returns:
        {
            "summary": "...",
            "key_points": [...],
            "recommendations": [...],
            "target_audience": "...",
            "classification": {
                "section": "...",
                "subsection": "...",
                "tags": [...]
            }
        }
    """
    
    prompt = load_prompt("config/prompts/summarizer.md")
    prompt = prompt.format(
        title=metadata.title,
        speaker=metadata.speaker,
        date=metadata.date.strftime("%d %B %Y"),
        stream=metadata.stream_full,
        transcript=cleaned_text
    )
    
    response = await client.chat(
        model="qwen2.5:14b",
        messages=[{"role": "user", "content": prompt}],
        options={"temperature": 0.5},
        format="json"
    )
    
    return json.loads(response["message"]["content"])
```

### –ü—Ä–æ–º–ø—Ç —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ (config/prompts/summarizer.md)

```markdown
–°–æ–∑–¥–∞–π —Å–∞–º–º–∞—Ä–∏ –æ–±—É—á–∞—é—â–µ–≥–æ –≤–∏–¥–µ–æ –¥–ª—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç–æ–≤ Herbalife.

**–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ:**
- –¢–µ–º–∞: {title}
- –°–ø–∏–∫–µ—Ä: {speaker}
- –î–∞—Ç–∞: {date}
- –ü–æ—Ç–æ–∫: {stream}

**–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç:**

{transcript}

**–°–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏ –≤ JSON:**

```json
{
  "summary": "2-3 –∞–±–∑–∞—Ü–∞: –æ —á—ë–º –≤–∏–¥–µ–æ –∏ –∫–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç",
  
  "key_points": [
    "–ö–ª—é—á–µ–≤–æ–π —Ç–µ–∑–∏—Å 1",
    "–ö–ª—é—á–µ–≤–æ–π —Ç–µ–∑–∏—Å 2",
    "–ö–ª—é—á–µ–≤–æ–π —Ç–µ–∑–∏—Å 3"
  ],
  
  "recommendations": [
    "–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 1",
    "–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 2",
    "–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 3"
  ],
  
  "target_audience": "–î–ª—è –∫–æ–≥–æ –ø–æ–ª–µ–∑–Ω–æ —ç—Ç–æ –≤–∏–¥–µ–æ",
  
  "classification": {
    "section": "–û–¥–∏–Ω –∏–∑: –û–±—É—á–µ–Ω–∏–µ | –ü—Ä–æ–¥—É–∫—Ç—ã | –ë–∏–∑–Ω–µ—Å | –ú–æ—Ç–∏–≤–∞—Ü–∏—è",
    "subsection": "–ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è –≤–Ω—É—Ç—Ä–∏ —Å–µ–∫—Ü–∏–∏",
    "tags": ["—Ç–µ–≥1", "—Ç–µ–≥2", "—Ç–µ–≥3", "—Ç–µ–≥4", "—Ç–µ–≥5"],
    "access_level": 1
  },
  
  "questions_answered": [
    "–ù–∞ –∫–∞–∫–æ–π –≤–æ–ø—Ä–æ—Å –æ—Ç–≤–µ—á–∞–µ—Ç –≤–∏–¥–µ–æ 1?",
    "–ù–∞ –∫–∞–∫–æ–π –≤–æ–ø—Ä–æ—Å –æ—Ç–≤–µ—á–∞–µ—Ç –≤–∏–¥–µ–æ 2?",
    "–ù–∞ –∫–∞–∫–æ–π –≤–æ–ø—Ä–æ—Å –æ—Ç–≤–µ—á–∞–µ—Ç –≤–∏–¥–µ–æ 3?"
  ]
}
```

**–û—Ç–≤–µ—Ç:**
```

### –ú–æ–¥–µ–ª—å –¥–∞–Ω–Ω—ã—Ö

```python
@dataclass
class VideoSummary:
    """–°–∞–º–º–∞—Ä–∏ –≤–∏–¥–µ–æ –¥–ª—è –ë–ó 2.0."""
    
    # –ö–æ–Ω—Ç–µ–Ω—Ç
    summary: str                      # –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
    key_points: list[str]             # –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–∑–∏—Å—ã
    recommendations: list[str]        # –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    target_audience: str              # –î–ª—è –∫–æ–≥–æ –ø–æ–ª–µ–∑–Ω–æ
    questions_answered: list[str]     # –í–æ–ø—Ä–æ—Å—ã, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–≤–µ—á–∞–µ—Ç
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    section: str                      # –û–±—É—á–µ–Ω–∏–µ / –ü—Ä–æ–¥—É–∫—Ç—ã / –ë–∏–∑–Ω–µ—Å / –ú–æ—Ç–∏–≤–∞—Ü–∏—è
    subsection: str                   # –ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è
    tags: list[str]                   # –¢–µ–≥–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞
    access_level: int                 # –£—Ä–æ–≤–µ–Ω—å –¥–æ—Å—Ç—É–ø–∞ (1-4)
```

---

## –≠—Ç–∞–ø 6: Save Files

### –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ

–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞—Ä—Ö–∏–≤.

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∞—Ä—Ö–∏–≤–∞

```
/archive/
‚îî‚îÄ‚îÄ {–≥–æ–¥}/
    ‚îî‚îÄ‚îÄ {–º–µ—Å—è—Ü}/
        ‚îî‚îÄ‚îÄ {—Ç–∏–ø}.{–ø–æ—Ç–æ–∫}/
            ‚îî‚îÄ‚îÄ {—Ç–µ–º–∞} ({—Å–ø–∏–∫–µ—Ä})/
                ‚îú‚îÄ‚îÄ {original_filename}.mp4      # –í–∏–¥–µ–æ
                ‚îú‚îÄ‚îÄ transcript_chunks.json       # –î–ª—è RAG
                ‚îú‚îÄ‚îÄ summary.md                   # –î–ª—è File Search
                ‚îî‚îÄ‚îÄ transcript_raw.txt           # Backup –æ—Ä–∏–≥–∏–Ω–∞–ª–∞
```

### –ì–µ–Ω–µ—Ä–∞—Ü–∏—è transcript_chunks.json

```python
def save_transcript_chunks(
    video_id: str,
    metadata: VideoMetadata,
    raw_transcript: RawTranscript,
    chunks: TranscriptChunks,
    archive_path: Path
) -> Path:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç chunks –≤ JSON –¥–ª—è RAG-–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.
    """
    
    data = {
        "video_id": video_id,
        "metadata": {
            "title": metadata.title,
            "speaker": metadata.speaker,
            "date": metadata.date.isoformat(),
            "stream": metadata.stream,
            "stream_name": metadata.stream_full,
            "duration_seconds": raw_transcript.duration_seconds,
            "language": raw_transcript.language,
            "whisper_model": raw_transcript.whisper_model,
            "processed_at": datetime.now().isoformat(),
        },
        "chunks": [
            {
                "id": chunk.id,
                "index": chunk.index,
                "topic": chunk.topic,
                "text": chunk.text,
                "word_count": chunk.word_count,
            }
            for chunk in chunks.chunks
        ],
        "statistics": {
            "total_chunks": chunks.total_chunks,
            "avg_chunk_size": chunks.avg_chunk_size,
        }
    }
    
    output_path = archive_path / "transcript_chunks.json"
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    return output_path
```

### –ì–µ–Ω–µ—Ä–∞—Ü–∏—è summary.md

```python
def save_summary_md(
    video_id: str,
    metadata: VideoMetadata,
    raw_transcript: RawTranscript,
    summary: VideoSummary,
    archive_path: Path
) -> Path:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç summary.md —Å YAML frontmatter –¥–ª—è –ë–ó 2.0.
    """
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    duration = format_duration(raw_transcript.duration_seconds)
    
    content = f'''---
# === –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è ===
video_id: "{video_id}"
title: "{metadata.title}"
type: "video_summary"

# === –ò—Å—Ç–æ—á–Ω–∏–∫ ===
speaker: "{metadata.speaker}"
date: "{metadata.date.isoformat()}"
stream: "{metadata.stream}"
stream_name: "{metadata.stream_full}"
duration: "{duration}"

# === –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–ª—è –ë–ó 2.0 ===
section: "{summary.section}"
subsection: "{summary.subsection}"
access_level: {summary.access_level}
tags:
{format_yaml_list(summary.tags)}

# === –°—Å—ã–ª–∫–∏ ===
transcript_file: "transcript_chunks.json"

# === –°–ª—É–∂–µ–±–Ω–æ–µ ===
created: "{datetime.now().isoformat()}"
llm_model: "qwen2.5:14b"
---

# {metadata.title}

**–°–ø–∏–∫–µ—Ä:** {metadata.speaker}  
**–î–∞—Ç–∞:** {metadata.date.strftime("%d %B %Y")}  
**–ü–æ—Ç–æ–∫:** {metadata.stream_full}

---

## –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

{summary.summary}

## –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–∑–∏—Å—ã

{format_bullet_list(summary.key_points)}

## –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

{format_numbered_list(summary.recommendations)}

## –î–ª—è –∫–æ–≥–æ –ø–æ–ª–µ–∑–Ω–æ

{summary.target_audience}

## –í–æ–ø—Ä–æ—Å—ã, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–≤–µ—á–∞–µ—Ç –≤–∏–¥–µ–æ

{format_bullet_list(summary.questions_answered)}

---

üìù **–ü–æ–ª–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è:** –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
'''
    
    output_path = archive_path / "summary.md"
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(content)
    
    return output_path
```

### –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ raw —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ (backup)

```python
def save_raw_transcript(
    raw_transcript: RawTranscript,
    archive_path: Path
) -> Path:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç —Å —Ç–∞–π–º-–∫–æ–¥–∞–º–∏.
    Backup –Ω–∞ —Å–ª—É—á–∞–π –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∏.
    """
    
    output_path = archive_path / "transcript_raw.txt"
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(raw_transcript.text_with_timestamps)
    
    return output_path
```

---

## –ü–æ–ª–Ω—ã–π Pipeline Flow

```python
async def process_video(video_path: Path) -> ProcessingResult:
    """
    –ü–æ–ª–Ω—ã–π pipeline –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ.
    """
    
    # 1. Parse filename
    metadata = parse_filename(video_path.name)
    video_id = generate_video_id(metadata)
    
    # 2. Transcribe
    raw_transcript = await transcribe(video_path, WHISPER_CONFIG)
    
    # 3. Clean
    glossary = load_glossary()
    text_with_glossary = apply_glossary(raw_transcript.full_text, glossary)
    cleaned_text = await llm_clean_transcript(text_with_glossary, metadata)
    
    # 4. Chunk
    chunks = await chunk_transcript(cleaned_text, metadata)
    
    # 5. Summarize
    summary = await summarize_transcript(cleaned_text, metadata)
    
    # 6. Save
    archive_path = create_archive_path(metadata)
    
    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –≤–∏–¥–µ–æ
    shutil.move(video_path, archive_path / video_path.name)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    save_transcript_chunks(video_id, metadata, raw_transcript, chunks, archive_path)
    save_summary_md(video_id, metadata, raw_transcript, summary, archive_path)
    save_raw_transcript(raw_transcript, archive_path)
    
    return ProcessingResult(
        video_id=video_id,
        archive_path=archive_path,
        chunks_count=len(chunks),
        duration=raw_transcript.duration_seconds,
    )
```

---

## Error Handling

### –¢–∏–ø—ã –æ—à–∏–±–æ–∫

| –≠—Ç–∞–ø | –û—à–∏–±–∫–∞ | –î–µ–π—Å—Ç–≤–∏–µ |
|------|--------|----------|
| 1. Parse | –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∏–º–µ–Ω–∏ | –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å, —É–≤–µ–¥–æ–º–∏—Ç—å |
| 2. Whisper | OOM (–Ω–µ—Ö–≤–∞—Ç–∫–∞ VRAM) | Retry —Å –º–µ–Ω—å—à–µ–π –º–æ–¥–µ–ª—å—é |
| 2. Whisper | Corrupted video | –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å, —É–≤–µ–¥–æ–º–∏—Ç—å |
| 3-5. LLM | Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω | Retry —Å backoff |
| 3-5. LLM | Invalid JSON response | Retry (–¥–æ 3 —Ä–∞–∑) |
| 6. Save | Disk full | –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å pipeline |

### Retry –ª–æ–≥–∏–∫–∞

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=60)
)
async def call_ollama_with_retry(prompt: str, **kwargs):
    """–í—ã–∑–æ–≤ Ollama —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º retry."""
    return await ollama_client.chat(...)
```

---

## –°–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã

- [architecture.md](architecture.md) ‚Äî —Å—Ö–µ–º–∞ —Å–∏—Å—Ç–µ–º—ã, –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
- [data-formats.md](data-formats.md) ‚Äî –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã —Ñ–∞–π–ª–æ–≤
- [llm-prompts.md](llm-prompts.md) ‚Äî –≤—Å–µ –ø—Ä–æ–º–ø—Ç—ã —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
