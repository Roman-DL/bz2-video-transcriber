# Тестирование и выбор LLM моделей

Методология автоматического тестирования и сравнения LLM моделей для задач pipeline (очистка, чанкирование, суммаризация).

## Тестовый скрипт

Скрипт `scripts/test_chunker_models.py` автоматически:
1. Парсит метаданные видео
2. Транскрибирует (Whisper)
3. Очищает транскрипт
4. Тестирует чанкирование на указанных моделях
5. Собирает метрики и выводит рекомендацию

### Конфигурация

```python
# В начале скрипта
MODELS_TO_TEST = [
    "qwen2.5:14b",
    "gemma2:9b",
    # Добавить другие модели из доступных на Ollama
]

VIDEO_FILENAME = "имя_файла.mp4"  # Тестовый файл из inbox
NUM_RUNS = 2  # Количество запусков для проверки стабильности
```

### Запуск

```bash
# 1. Скопировать скрипт на сервер
export $(grep -v '^#' .env.local | xargs) && \
sshpass -p "$DEPLOY_PASSWORD" scp scripts/test_chunker_models.py \
  "$DEPLOY_USER@$DEPLOY_HOST:/tmp/"

# 2. Скопировать в контейнер
sshpass -p "$DEPLOY_PASSWORD" ssh "$DEPLOY_USER@$DEPLOY_HOST" \
  "echo '$DEPLOY_PASSWORD' | sudo -S docker cp /tmp/test_chunker_models.py bz2-transcriber:/tmp/"

# 3. Запустить тест (10-15 минут)
sshpass -p "$DEPLOY_PASSWORD" ssh "$DEPLOY_USER@$DEPLOY_HOST" \
  "echo '$DEPLOY_PASSWORD' | sudo -S docker exec bz2-transcriber \
   python3 /tmp/test_chunker_models.py"
```

---

## Метрики оценки

### Для чанкирования

| Метрика | Описание | Идеал |
|---------|----------|-------|
| Success rate | % успешных запусков без ошибок JSON | 100% |
| Chunks count | Количество чанков | 10-20 для ~3000 слов |
| Avg chunk size | Средний размер чанка в словах | 150-300 |
| Normal ratio | % чанков в диапазоне 100-400 слов | >80% |
| Time | Время обработки | Меньше лучше |

### Для очистки транскрипта

| Метрика | Описание | Идеал |
|---------|----------|-------|
| Compression % | Сокращение текста | 20-40% |
| JSON validity | Корректность ответа | 100% |
| Text quality | Сохранение смысла | Субъективно |

### Для суммаризации

| Метрика | Описание | Идеал |
|---------|----------|-------|
| JSON validity | Корректность структуры | 100% |
| Completeness | Покрытие тем | Все ключевые темы |
| Coherence | Связность текста | Читаемый текст |

---

## Формула оценки (chunking)

```python
# Используется в test_chunker_models.py
success_rate = len(successes) / len(runs)
avg_chunks = mean([r["chunks"] for r in successes])
avg_normal_ratio = mean([r["normal_chunks"] / r["chunks"] for r in successes])

# Штраф за отклонение от идеала (15 чанков для ~3000 слов)
chunk_penalty = abs(avg_chunks - 15) / 15

# Итоговый score (0-1)
score = success_rate * (1 - chunk_penalty * 0.5) * (0.5 + avg_normal_ratio * 0.5)
```

---

## Результаты тестов

### Чанкирование (Январь 2026)

Тест на файле 55 минут (~3300 слов после очистки):

| Модель | Success | Chunks | Avg Size | Normal% | Score |
|--------|---------|--------|----------|---------|-------|
| qwen2.5:14b | 100% | 3 | 243 | 100% | 0.60 |
| **gemma2:9b** | 100% | 13 | 176 | 85% | **0.87** |
| phi3:14b | 0% | - | - | - | 0 |
| mistral:7b | 100% | 1 | 150 | 100% | ~0.3 |

**Выбор:** gemma2:9b - оптимальное количество и размер чанков для RAG.

**Проблема qwen2.5:14b:** Создаёт слишком крупные чанки (3 чанка на 3300 слов = ~1100 слов/чанк), что снижает точность поиска.

### Очистка транскрипта

| Модель | Compression | Стабильность | Примечание |
|--------|-------------|--------------|------------|
| **gemma2:9b** | 20-30% | Высокая | Рекомендуется |
| qwen2.5:14b | 50-60% | Средняя | Слишком агрессивное сокращение |

---

## Текущая конфигурация

```python
# backend/app/config.py
llm_model: str = "qwen2.5:14b"      # Для суммаризации
cleaner_model: str = "gemma2:9b"    # Для очистки
chunker_model: str = "gemma2:9b"    # Для чанкирования
```

---

## Когда проводить тесты

1. **При добавлении новой модели** в Ollama
2. **При изменении промптов** в `config/prompts/`
3. **При изменении параметров** чанкирования/очистки
4. **При обновлении Ollama** или базовых моделей
5. **При появлении ошибок** JSON parsing в production

---

## Добавление новой модели

1. Установить модель на сервере:
   ```bash
   ssh user@server "ollama pull model_name:tag"
   ```

2. Добавить в `MODELS_TO_TEST` в скрипте

3. Запустить тесты

4. Обновить конфиг если новая модель лучше:
   ```python
   # backend/app/config.py
   chunker_model: str = "new_model:tag"
   ```

5. Задеплоить: `./scripts/deploy.sh`

6. Обновить результаты в этом документе

---

## Доступные модели (Январь 2026)

```bash
# Проверить на сервере
curl -s http://100.64.0.1:11434/api/tags | jq '.models[].name'
```

| Модель | Размер | Назначение |
|--------|--------|------------|
| qwen2.5:14b | 9.0GB | Суммаризация |
| gemma2:9b | 5.4GB | Очистка, чанкирование |
| phi3:14b | 7.9GB | Не рекомендуется (JSON ошибки) |
| mistral:7b-instruct | 4.4GB | Быстрые задачи |
| qwen2.5:7b | 4.7GB | Лёгкая альтернатива |

---

## Лимиты контекста моделей

При работе с LLM важно учитывать размер контекста — максимальное количество токенов в запросе.

| Модель | Контекст | ~Символов | Примечание |
|--------|----------|-----------|------------|
| gemma2:9b | 8192 токена | ~24K | Используется для chunking/cleaning |
| qwen2.5:14b | 32K токена | ~96K | Используется для summarization |
| mistral:7b | 8K токена | ~24K | — |

**Практические следствия:**

1. **PART_SIZE = 6000 символов** (~2000 токенов) — оставляет место для промпта (~1500 токенов)
2. **Whisper-артефакты:** Модель ставит запятые вместо точек → TextSplitter разбивает по запятым
3. **Диагностика:** При пустом ответе LLM логируется размер промпта в токенах

**Формула оценки:**
```
токены ≈ символы / 3
промпт = системный_промпт + текст + few-shot_примеры
```

---

## Связанные документы

- [testing.md](testing.md) — базовое тестирование pipeline
- [architecture.md](architecture.md) — архитектура системы
- [pipeline/](pipeline/) — документация этапов обработки
