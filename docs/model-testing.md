---
doc_type: how-to
status: active
updated: 2026-01-24
audience: [developer, ai-agent]
tags:
  - testing
  - llm
  - configuration
---

# Тестирование и выбор LLM моделей

Методология автоматического тестирования и сравнения LLM моделей для задач pipeline (очистка, суммаризация, лонгрид).

> **v0.26+:** Чанкирование теперь детерминистическое (парсинг H2 заголовков из longread/story), LLM не используется.

## Тестовый скрипт

Скрипт `scripts/test_models_comprehensive.py` автоматически:
1. Парсит метаданные видео
2. Транскрибирует (Whisper)
3. Очищает транскрипт
4. Генерирует лонгрид/story
5. Собирает метрики и выводит рекомендацию

---

## Метрики оценки

### Для очистки транскрипта

| Метрика | Описание | Идеал |
|---------|----------|-------|
| Compression % | Сокращение текста | 20-40% |
| JSON validity | Корректность ответа | 100% |
| Text quality | Сохранение смысла | Субъективно |

### Для суммаризации / лонгрида

| Метрика | Описание | Идеал |
|---------|----------|-------|
| JSON validity | Корректность структуры | 100% |
| Completeness | Покрытие тем | Все ключевые темы |
| Coherence | Связность текста | Читаемый текст |

---

## Результаты тестов

### Очистка транскрипта

| Модель | Compression | Стабильность | Примечание |
|--------|-------------|--------------|------------|
| **gemma2:9b** | 20-30% | Высокая | Рекомендуется |
| qwen2.5:14b | 50-60% | Средняя | Слишком агрессивное сокращение |

### Лонгрид / Конспект

| Модель | Качество | Стабильность | Примечание |
|--------|----------|--------------|------------|
| **qwen2.5:14b** | Высокое | Высокая | Рекомендуется для лонгрида |
| gemma2:9b | Среднее | Высокая | Ограничен контекстом |

---

## Текущая конфигурация

```python
# backend/app/config.py
summarizer_model: str = "qwen2.5:14b"  # Для суммаризации
cleaner_model: str = "gemma2:9b"       # Для очистки
longread_model: str = "qwen2.5:14b"    # Для лонгрида
# chunking: детерминистический (H2 парсинг), модель не требуется
```

---

## Когда проводить тесты

1. **При добавлении новой модели** в Ollama
2. **При изменении промптов** в `config/prompts/`
3. **При обновлении Ollama** или базовых моделей
4. **При появлении ошибок** JSON parsing в production

---

## Добавление новой модели

1. Установить модель на сервере:
   ```bash
   ssh user@server "ollama pull model_name:tag"
   ```

2. Обновить конфиг если новая модель лучше:
   ```python
   # backend/app/config.py
   cleaner_model: str = "new_model:tag"
   ```

3. Задеплоить: `./scripts/deploy.sh`

4. Обновить результаты в этом документе

---

## Доступные модели (Январь 2026)

```bash
# Проверить на сервере
curl -s http://100.64.0.1:11434/api/tags | jq '.models[].name'
```

| Модель | Размер | Назначение |
|--------|--------|------------|
| qwen2.5:14b | 9.0GB | Суммаризация, лонгрид |
| gemma2:9b | 5.4GB | Очистка транскрипта |
| phi3:14b | 7.9GB | Не рекомендуется (JSON ошибки) |
| mistral:7b-instruct | 4.4GB | Быстрые задачи |
| qwen2.5:7b | 4.7GB | Лёгкая альтернатива |

---

## Лимиты контекста моделей

При работе с LLM важно учитывать размер контекста — максимальное количество токенов в запросе.

| Модель | Контекст | ~Символов | Примечание |
|--------|----------|-----------|------------|
| gemma2:9b | 8192 токена | ~24K | Используется для cleaning |
| qwen2.5:14b | 32K токена | ~96K | Используется для summarization/longread |
| mistral:7b | 8K токена | ~24K | — |

**Практические следствия:**

1. **PART_SIZE = 6000 символов** (~2000 токенов) — оставляет место для промпта (~1500 токенов)
2. **Whisper-артефакты:** Модель ставит запятые вместо точек → TextSplitter разбивает по запятым
3. **Диагностика:** При пустом ответе LLM логируется размер промпта в токенах

**Формула оценки:**
```
токены ≈ символы / 3
промпт = системный_промпт + текст + few-shot_примеры
```

---

## Связанные документы

- [testing.md](testing.md) — базовое тестирование pipeline
- [ARCHITECTURE.md](ARCHITECTURE.md) — архитектура системы
- [pipeline/](pipeline/) — документация этапов обработки
