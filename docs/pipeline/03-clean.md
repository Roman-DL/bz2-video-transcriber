# Этап 3: Clean (Очистка транскрипта)

[< Назад: Transcribe](02-transcribe.md) | [Обзор Pipeline](README.md) | [Далее: Chunk >](04-chunk.md)

---

## Назначение

Очистка сырого транскрипта от речевого мусора и нормализация терминологии с использованием LLM.

## Проблемы сырого транскрипта

| Проблема | Пример | Решение |
|----------|--------|---------|
| Слова-паразиты | "ну", "вот", "как бы", "эээ" | LLM удаляет |
| Отвлечения | "видно экран?", "вы меня слышите?" | LLM удаляет |
| Ошибки Whisper | "Формула один", "херболайф" | LLM исправляет по глоссарию |
| Термины Herbalife | "гербалайф", "СВ", "гет тим" | LLM нормализует по контексту |

## Архитектура очистки (v0.22+)

```
RawTranscript (~70KB контента для 55-мин видео)
       │
       ▼
┌─────────────────┐
│ 1. CHUNKING     │  Разбиение на части ~3KB
│    (~24 чанка)  │  (для стабильной обработки)
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ 2. LLM CLEAN    │  Chat API с system/user roles
│    (по частям)  │  (glossary.yaml как контекст)
│    (gemma2:9b)  │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ 3. MERGE        │  Склейка результатов без дублирования
│                 │
└────────┬────────┘
         │
         ▼
  CleanedTranscript (ожидание: ~60KB, ~15% reduction)
```

## Глоссарий как контекст LLM

**Изменение в v0.22:** Глоссарий передаётся в LLM как контекст вместо regex-замен.

### Почему LLM вместо Regex

| Аспект | Regex (старый подход) | LLM с глоссарием (новый) |
|--------|----------------------|--------------------------|
| Распознавание | Только точные совпадения | По смыслу и звучанию |
| "херболайф" | ❌ Не найдёт | ✅ Распознает как "Herbalife" |
| "формула первая" | ❌ Не найдёт | ✅ Поймёт из description |
| "супер визор" | ❌ Не найдёт | ✅ По контексту роли |
| Поддержка | Добавлять вариации | LLM догадается сам |

### Структура промпта

```
┌────────────────────────┐
│ System prompt          │  — инструкции по очистке
│ (cleaner_system.md)    │  — правила работы с глоссарием
└────────────────────────┘
┌────────────────────────┐
│ User prompt            │
│ ┌──────────────────┐   │
│ │ <glossary>       │   │  — glossary.yaml полностью
│ │ ...              │   │
│ │ </glossary>      │   │
│ ├──────────────────┤   │
│ │ <input>          │   │  — транскрипт для очистки
│ │ ...              │   │
│ │ </input>         │   │
│ └──────────────────┘   │
└────────────────────────┘
```

### Что модель делает с глоссарием

Для каждого термина в `glossary.yaml` есть:
- `canonical` — правильное написание
- `description` — смысл термина
- `variations` — известные ошибки транскрибации
- `context` — пример употребления

LLM использует эту информацию для:
1. Замены точных совпадений с `variations`
2. Распознавания **неизвестных** вариаций по звучанию
3. Понимания контекста использования термина

### Примеры распознавания

```
"херболайф" → "Herbalife"     (похоже на variations по звучанию)
"формула первая" → "Формула 1" (по описанию из description)
"супер визор" → "Супервайзер"  (по смыслу и роли)
"генты" → "ГЕТ"               (из variations списка)
```

## Текущие параметры

| Параметр | Значение | Описание |
|----------|----------|----------|
| `CLEANER_MODEL` | gemma2:9b | Модель для очистки (env variable) |
| `CHUNK_SIZE_CHARS` | 3000 | Размер одной части |
| `CHUNK_OVERLAP_CHARS` | 200 | Перекрытие между частями |
| `SMALL_TEXT_THRESHOLD` | 3500 | Порог для включения chunking |
| `temperature` | 0.0 | Детерминированный вывод |

## Выбор модели

Модель `gemma2:9b` выбрана по результатам тестирования:

| Модель | Reduction на 3KB | Reduction на 6KB | Статус |
|--------|------------------|------------------|--------|
| gemma2:9b | 18.0% | 19.7% | ✅ Стабильна |
| mistral:7b-instruct | 18.4% | 71.4% | ❌ Нестабильна |
| phi3:14b | 48.1% | — | ❌ Суммаризирует |
| qwen2.5:14b | — | 85% | ❌ Суммаризирует |

**Критерии выбора:**
- Reduction 10-20% (не суммаризирует)
- Стабильность на разных размерах чанков
- Качество русского языка

---

## Chat API с system/user roles

Используем Chat API с разделением на роли:
- `system` — инструкции по очистке и работе с глоссарием
- `user` — глоссарий + транскрипт для обработки

```python
messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": user_template.format(
        glossary=glossary_text,
        transcript=chunk,
    )},
]
result = await ai_client.chat(messages, model=settings.cleaner_model, temperature=0.0)
```

## Ожидаемые показатели

| Тип спикера | Сокращение | Примечание |
|-------------|------------|------------|
| Опытный | 5-10% | Подготовленное выступление |
| Средний | 10-15% | Вебинар, живое общение |
| Неопытный | 15-20% | Много запинок, техпроблемы |

**Важно:** Сокращение >25% означает возможную потерю контента. Сокращение >40% — скорее всего, LLM сделал саммари вместо очистки.

## Валидация результата

Сервис автоматически проверяет результат очистки на **двух уровнях**:

### 1. Валидация каждого чанка

После обработки каждого чанка проверяется:
- **Reduction >40%** — модель вероятно суммаризировала
- **Кириллица <50%** — модель переключилась на английский

При провале валидации — **fallback на оригинальный текст** (лучше с паразитами, чем потерять контент).

```
INFO:  Chunk 1/16: 3000 -> 2400 chars (20% reduction), cyrillic=98%   ← OK
WARN:  Chunk 5 high reduction! Input start: ... | Output start: ...  ← Проверить
ERROR: Chunk 5 FAILED validation: reduction=56%, cyrillic=45%.       ← Fallback!
       Using original text instead.
```

### 2. Валидация итогового результата

```
INFO:  Cleaning complete: 47233 -> 40000 chars (15% reduction)  ← OK
WARN:  High reduction: 30% - possible content loss              ← Проверить
ERROR: Suspicious reduction: 85% - likely summarization         ← Баг!
```

### Логирование merge

При слиянии чанков логируется процент удалённых перекрытий:

```
INFO: Pre-merge stats: total input=48000, total output=40800, overall reduction=15%
INFO: Merge: 40800 -> 39800 chars (2% removed as overlap)
```

Высокий процент при merge (>5%) указывает на возможные проблемы со слиянием.

## Тестирование

```bash
cd backend
python -m app.services.cleaner
```

Тесты проверяют: загрузку glossary_text, загрузку промптов с placeholder'ами, полную очистку с LLM.

---

## Связанные файлы

- **Код:** [backend/app/services/cleaner.py](../../backend/app/services/cleaner.py)
- **System prompt:** [config/prompts/cleaner_system.md](../../config/prompts/cleaner_system.md)
- **User template:** [config/prompts/cleaner_user.md](../../config/prompts/cleaner_user.md)
- **Глоссарий:** [config/glossary.yaml](../../config/glossary.yaml)
- **AI клиенты:** [backend/app/services/ai_clients/](../../backend/app/services/ai_clients/)
- **Исследование:** [docs/research/llm-transcript-cleaning-guide.md](../research/llm-transcript-cleaning-guide.md)
