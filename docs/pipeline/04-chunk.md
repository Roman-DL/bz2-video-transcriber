# Этап 4: Chunk (Semantic Splitting)

[< Назад: Clean](03-clean.md) | [Обзор Pipeline](README.md) | [Далее: Summarize >](05-summarize.md)

---

## Назначение

Разбиение очищенного транскрипта на смысловые блоки для RAG-поиска в БЗ 2.0.

## Архитектура

LLM разбивает текст на самодостаточные блоки. Самодостаточность обеспечивается инструкциями в промпте.

| Критерий | Значение | Почему |
|----------|----------|--------|
| Размер chunk | 100-400 слов (оптимум 200-300) | Оптимально для embeddings |
| Смысловая завершённость | Одна тема/мысль | Chunk понятен без контекста |
| Overlap | Не требуется | LLM делает чанки самодостаточными |
| Метаданные | topic + text + word_count | Минимум для простоты и надёжности |

### Модель данных

Каждый чанк содержит:
- **id** — уникальный идентификатор (формат: `{video_id}_{index:03d}`)
- **topic** — краткая тема блока (3-7 слов)
- **text** — полный текст блока
- **word_count** — количество слов (вычисляется автоматически)

### Обработка больших текстов

Для транскриптов > 10,000 символов текст разбивается на части перед отправкой в LLM.

**Почему:** Модель qwen2.5:14b имеет ограниченный контекст. При отправке большого текста целиком LLM возвращает пустой ответ или невалидный JSON.

**Как работает:**
1. Текст разбивается на части по ~8,000 символов (по границам предложений)
2. Каждая часть обрабатывается отдельно
3. Чанки из всех частей собираются в единый список
4. Мелкие чанки (< 50 слов) объединяются с соседними

### Валидация размера чанков

LLM иногда игнорирует требования к размеру и создаёт чанки по 10-30 слов. После получения ответа:
- Чанки < 50 слов объединяются с соседними
- Индексы перенумеровываются

---

## Ограничения текущего подхода

> **Важно:** Разбиение текста на части перед отправкой в LLM — это workaround для ограничений контекста модели, а не оптимальное решение.

**Проблемы при обработке больших текстов:**
- LLM видит только часть текста → не может определить глобальные темы
- Границы между частями могут разрезать логически связанный контент
- Результат chunking может быть не оптимальным по семантике

**Когда это критично:**
- Видео > 30 минут с переплетающимися темами
- Спикер часто возвращается к ранее затронутым темам
- Важна точная тематическая классификация для RAG-поиска

---

## Альтернативные подходы

Текущая реализация выбрана за баланс простоты и качества. Ниже — альтернативы для будущего рассмотрения.

### Вариант B: Чанки с контекстом

Добавить поле `context` с описанием места чанка в общей теме видео.

| Плюсы | Минусы |
|-------|--------|
| Лучше для RAG-поиска | Сложнее промпт → больше ошибок LLM |
| Чанк понятен автономно | Больше токенов → медленнее |

**Когда использовать:** Если при тестировании RAG-поиска чанки недостаточно информативны.

### Вариант C: Двухэтапная обработка

Сначала LLM создаёт "план" транскрипта, затем разбивает на чанки со ссылкой на план.

| Плюсы | Минусы |
|-------|--------|
| Максимальная точность | 2 вызова LLM вместо 1 |
| Понимание общей структуры | Сильно усложняет реализацию |

**Когда использовать:** Если видео >1 час и спикер часто возвращается к темам.

### Вариант D: Гибридный (LLM + эвристики)

LLM определяет границы тем, финальное разбиение — программно по размеру.

| Плюсы | Минусы |
|-------|--------|
| Контроль размера чанков | Может разрезать середину мысли |
| Предсказуемый результат | Теряем "умное" разбиение LLM |

**Когда использовать:** Если нужен строгий контроль размера для RAG.

### Вариант E: Модель с большим контекстом

Использовать LLM с контекстом 128K+ токенов для обработки всего транскрипта целиком.

| Плюсы | Минусы |
|-------|--------|
| LLM видит весь текст | Требует другую модель/инфраструктуру |
| Оптимальное семантическое разбиение | Медленнее на больших текстах |

**Когда использовать:** Если качество семантики критично и есть ресурсы на инфраструктуру.

---

## Тестирование

```bash
python -m backend.app.services.chunker
```

Тесты проверяют: загрузку промпта, извлечение JSON, парсинг чанков, интеграцию с LLM.

---

## Связанные файлы

- **Сервис:** `backend/app/services/chunker.py`
- **Модели:** `backend/app/models/schemas.py`
- **Промпт:** `config/prompts/chunker.md`
