"""
Summary generator service.

Creates condensed summaries (конспект) from cleaned transcripts.
A summary is a navigation document for those who already watched/read the content.

v0.24+: Generates from CleanedTranscript (not Longread) using 3-component prompt architecture.
"""

import json
import logging
import re
import time
from typing import Any

from app.config import Settings, get_settings, load_prompt, get_model_config
from app.utils.json_utils import extract_json
from app.models.schemas import (
    CleanedTranscript,
    Summary,
    VideoMetadata,
)
from app.services.ai_clients import BaseAIClient, OllamaClient

logger = logging.getLogger(__name__)
perf_logger = logging.getLogger("app.perf")

# Russian month names for date formatting
RUSSIAN_MONTHS = [
    "", "января", "февраля", "марта", "апреля", "мая", "июня",
    "июля", "августа", "сентября", "октября", "ноября", "декабря"
]

# Valid topic_area values for classification
VALID_TOPIC_AREAS = [
    "продажи", "спонсорство", "лидерство",
    "мотивация", "инструменты", "маркетинг-план"
]

# Valid access_level values
VALID_ACCESS_LEVELS = ["consultant", "leader", "personal"]

# Default max input chars (for truncation if needed)
DEFAULT_MAX_INPUT_CHARS = 50000


class SummaryGenerator:
    """
    Summary generation service.

    Creates a condensed summary (конспект) from cleaned transcript.
    The summary helps those who already watched/read the content to
    quickly recall key points.

    v0.24+: Uses 3-component prompt architecture (system + instructions + template).
    LLM generates topic_area, tags, and access_level.

    Example:
        async with OllamaClient.from_settings(settings) as client:
            generator = SummaryGenerator(client, settings)
            summary = await generator.generate(cleaned_transcript, metadata)
            markdown = summary.to_markdown()
    """

    def __init__(self, ai_client: BaseAIClient, settings: Settings):
        """
        Initialize summary generator.

        Args:
            ai_client: AI client for LLM calls
            settings: Application settings
        """
        self.ai_client = ai_client
        self.settings = settings

        # Load 3-component prompt architecture (v0.30+: hierarchical structure)
        self.system_prompt = load_prompt(
            "summary", "system", settings.summarizer_model, settings
        )
        self.instructions = load_prompt(
            "summary", "instructions", settings.summarizer_model, settings
        )
        self.template = load_prompt(
            "summary", "template", settings.summarizer_model, settings
        )

        # Get model-specific config
        model_config = get_model_config(settings.summarizer_model, settings)
        summary_config = model_config.get("summary", {})
        self.max_input_chars = summary_config.get(
            "max_input_chars", DEFAULT_MAX_INPUT_CHARS
        )

        logger.debug(f"SummaryGenerator config: max_input_chars={self.max_input_chars}")

    async def generate(
        self,
        cleaned_transcript: CleanedTranscript,
        metadata: VideoMetadata,
    ) -> Summary:
        """
        Generate summary from cleaned transcript.

        Args:
            cleaned_transcript: Cleaned transcript text
            metadata: Video metadata

        Returns:
            Summary with essence, concepts, tools, quotes, insight, actions,
            topic_area, tags, access_level (all generated by LLM)
        """
        start_time = time.time()

        # Prepare transcript text
        transcript_text = self._prepare_transcript_text(cleaned_transcript)
        input_chars = len(transcript_text)

        logger.info(
            f"Generating summary from cleaned transcript: {input_chars} chars"
        )

        # Build prompt
        prompt = self._build_prompt(transcript_text, metadata)

        # Call LLM
        response = await self.ai_client.generate(
            prompt, model=self.settings.summarizer_model
        )

        # Parse response
        summary_data = self._parse_response(response)

        elapsed = time.time() - start_time

        # Validate and normalize classification
        topic_area = self._validate_topic_area(summary_data.get("topic_area", []))
        access_level = self._validate_access_level(summary_data.get("access_level"))

        summary = Summary(
            video_id=metadata.video_id,
            title=metadata.title,
            speaker=metadata.speaker,
            date=metadata.date,
            essence=summary_data.get("essence", ""),
            key_concepts=summary_data.get("key_concepts", []),
            practical_tools=summary_data.get("practical_tools", []),
            quotes=summary_data.get("quotes", []),
            insight=summary_data.get("insight", ""),
            actions=summary_data.get("actions", []),
            # LLM-generated classification
            topic_area=topic_area,
            tags=summary_data.get("tags", []),
            access_level=access_level,
            model_name=self.settings.summarizer_model,
        )

        logger.info(
            f"Summary complete: {len(summary.key_concepts)} concepts, "
            f"{len(summary.quotes)} quotes, topic_area={topic_area}, {elapsed:.1f}s"
        )

        perf_logger.info(
            f"PERF | summary | "
            f"input_chars={input_chars} | "
            f"concepts={len(summary.key_concepts)} | "
            f"quotes={len(summary.quotes)} | "
            f"topic_area={','.join(topic_area)} | "
            f"time={elapsed:.1f}s"
        )

        return summary

    def _prepare_transcript_text(self, cleaned_transcript: CleanedTranscript) -> str:
        """
        Prepare cleaned transcript text for prompt.

        If the transcript is too large, truncate it intelligently.

        Args:
            cleaned_transcript: Cleaned transcript

        Returns:
            Text ready for prompt insertion
        """
        text = cleaned_transcript.text

        # Truncate if needed
        if len(text) > self.max_input_chars:
            logger.warning(
                f"Transcript too large ({len(text)} chars), "
                f"truncating to {self.max_input_chars}"
            )
            text = self._truncate_text(text, self.max_input_chars)

        return text

    def _truncate_text(self, text: str, max_chars: int) -> str:
        """
        Truncate text intelligently, preserving structure.

        Tries to cut at paragraph boundaries rather than mid-sentence.

        Args:
            text: Full text
            max_chars: Maximum characters

        Returns:
            Truncated text with indication that it was cut
        """
        if len(text) <= max_chars:
            return text

        # Find a good cut point (at a paragraph or sentence)
        cut_point = max_chars - 100  # Leave room for truncation notice

        # Try to find last paragraph break before cut point
        last_para = text.rfind("\n\n", 0, cut_point)
        if last_para > max_chars // 2:
            return text[:last_para] + "\n\n[... сокращено для обработки ...]"

        # Try to find last sentence break
        last_sentence = max(
            text.rfind(". ", 0, cut_point),
            text.rfind("! ", 0, cut_point),
            text.rfind("? ", 0, cut_point),
        )
        if last_sentence > max_chars // 2:
            return text[:last_sentence + 1] + "\n\n[... сокращено для обработки ...]"

        # Fall back to hard cut
        return text[:cut_point] + "...\n\n[... сокращено для обработки ...]"

    def _build_prompt(self, transcript_text: str, metadata: VideoMetadata) -> str:
        """
        Build summary prompt from 3 components.

        Args:
            transcript_text: Prepared transcript text
            metadata: Video metadata

        Returns:
            Complete prompt for LLM
        """
        date_formatted = f"{metadata.date.day} {RUSSIAN_MONTHS[metadata.date.month]} {metadata.date.year}"

        prompt_parts = [
            self.system_prompt,
            "",
            "---",
            "",
            self.instructions,
            "",
            "---",
            "",
            "## Задание",
            "",
            "Создай конспект по транскрипту выступления.",
            "",
            f"**Спикер:** {metadata.speaker}",
            f"**Тема:** {metadata.title}",
            f"**Дата:** {date_formatted}",
            f"**Событие:** {metadata.event_type}",
            "",
            "### Транскрипт",
            "",
            transcript_text,
            "",
            "### Формат ответа",
            "",
            self.template,
        ]

        return "\n".join(prompt_parts)

    def _parse_response(self, response: str) -> dict[str, Any]:
        """
        Parse LLM response into summary data.

        Args:
            response: Raw LLM response

        Returns:
            Dict with summary fields
        """
        json_str = extract_json(response, json_type="object")

        try:
            data = json.loads(json_str)
            if not isinstance(data, dict):
                logger.error(f"Expected dict, got {type(data).__name__}")
                return {}
            return data
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON: {e}")
            logger.debug(f"Response was: {response[:500]}...")
            return {}

    def _validate_topic_area(self, topic_area: Any) -> list[str]:
        """
        Validate and normalize topic_area.

        Args:
            topic_area: Raw value from LLM (string or list)

        Returns:
            List of valid topic areas
        """
        # Handle string input
        if isinstance(topic_area, str):
            topic_area = [topic_area]

        # Handle non-list input
        if not isinstance(topic_area, list):
            return ["мотивация"]  # Default

        # Filter to valid values only
        valid = [t for t in topic_area if t in VALID_TOPIC_AREAS]

        # Default if empty
        if not valid:
            return ["мотивация"]

        return valid

    def _validate_access_level(self, access_level: Any) -> str:
        """
        Validate and normalize access_level.

        Args:
            access_level: Raw value from LLM

        Returns:
            Valid access level string
        """
        if access_level in VALID_ACCESS_LEVELS:
            return access_level
        return "consultant"  # Default


if __name__ == "__main__":
    """Run tests when executed directly."""
    import asyncio
    import sys
    from datetime import date
    from pathlib import Path

    logging.basicConfig(level=logging.INFO)

    async def run_tests():
        """Run all summary generator tests."""
        print("\nRunning summary generator tests (v0.24)...\n")

        settings = get_settings()

        # Test 1: Load prompts (v0.30+ hierarchical structure)
        print("Test 1: Load prompts (v0.30+ hierarchical)...", end=" ")
        try:
            system_prompt = load_prompt("summary", "system", settings.summarizer_model, settings)
            instructions = load_prompt("summary", "instructions", settings.summarizer_model, settings)
            template = load_prompt("summary", "template", settings.summarizer_model, settings)
            assert "навигационный документ" in system_prompt
            assert "topic_area" in instructions
            assert "JSON" in template
            print("OK")
        except Exception as e:
            print(f"FAILED: {e}")
            return 1

        # Test 2: Text truncation
        print("\nTest 2: Text truncation...", end=" ")
        try:
            generator = SummaryGenerator(None, settings)  # type: ignore
            generator.max_input_chars = 100

            long_text = "A" * 200
            truncated = generator._truncate_text(long_text, 100)
            assert len(truncated) <= 100 + 50  # Some room for truncation notice
            assert "сокращено" in truncated
            print("OK")
        except Exception as e:
            print(f"FAILED: {e}")
            return 1

        # Test 3: topic_area validation
        print("\nTest 3: topic_area validation...", end=" ")
        try:
            generator = SummaryGenerator(None, settings)  # type: ignore

            # Test string input
            assert generator._validate_topic_area("продажи") == ["продажи"]

            # Test list input
            assert generator._validate_topic_area(["продажи", "мотивация"]) == ["продажи", "мотивация"]

            # Test invalid values filtered
            assert generator._validate_topic_area(["invalid", "продажи"]) == ["продажи"]

            # Test empty/invalid returns default
            assert generator._validate_topic_area([]) == ["мотивация"]
            assert generator._validate_topic_area("invalid") == ["мотивация"]

            print("OK")
        except Exception as e:
            print(f"FAILED: {e}")
            return 1

        # Test 4: access_level validation
        print("\nTest 4: access_level validation...", end=" ")
        try:
            generator = SummaryGenerator(None, settings)  # type: ignore

            assert generator._validate_access_level("consultant") == "consultant"
            assert generator._validate_access_level("leader") == "leader"
            assert generator._validate_access_level("personal") == "personal"
            assert generator._validate_access_level("invalid") == "consultant"
            assert generator._validate_access_level(None) == "consultant"

            print("OK")
        except Exception as e:
            print(f"FAILED: {e}")
            return 1

        # Test 5: JSON extraction using shared utils
        print("\nTest 5: JSON extraction (shared utils)...", end=" ")
        try:
            test_json = '```json\n{"essence": "Test", "quotes": [], "topic_area": ["продажи"]}\n```'
            result = extract_json(test_json, json_type="object")
            parsed = json.loads(result)
            assert "essence" in parsed
            assert "topic_area" in parsed
            print("OK")
        except Exception as e:
            print(f"FAILED: {e}")
            return 1

        # Test 6: Full generation (requires Ollama)
        print("\nTest 6: Full generation from CleanedTranscript...", end=" ")
        async with OllamaClient.from_settings(settings) as client:
            status = await client.check_services()

            if not status["ollama"]:
                print("SKIPPED (Ollama unavailable)")
            else:
                try:
                    generator = SummaryGenerator(client, settings)

                    mock_metadata = VideoMetadata(
                        date=date(2025, 1, 20),
                        event_type="ПШ",
                        stream="SV",
                        title="Работа с клиентами",
                        speaker="Тест Спикер",
                        original_filename="test.mp4",
                        video_id="test-video",
                        source_path=Path("/test/test.mp4"),
                        archive_path=Path("/archive/test"),
                    )

                    mock_cleaned = CleanedTranscript(
                        text=(
                            "Сегодня мы поговорим о важной теме работы с клиентами. "
                            "Первое, что нужно понять — клиент всегда ищет решение своей проблемы. "
                            "Я использую инструмент под названием дом-магазин. "
                            "Суть его в том, чтобы показать человеку два пути: "
                            "покупать в магазине или покупать дома со скидкой. "
                            "«Главное — не продавать, а помогать людям» — это мой принцип. "
                            "Когда вы помогаете искренне, продажи случаются сами. "
                            "Вот пример: одна моя клиентка начала с программы похудения, "
                            "а через год стала СТ. Потому что я не давил, а помогал. "
                            "Запомните: работа с клиентами — это прежде всего работа над собой."
                        ),
                        original_length=800,
                        cleaned_length=700,
                        model_name="test",
                    )

                    summary = await generator.generate(mock_cleaned, mock_metadata)

                    assert summary.essence, "Essence is empty"
                    assert isinstance(summary.topic_area, list), "topic_area should be list"
                    assert all(t in VALID_TOPIC_AREAS for t in summary.topic_area), "Invalid topic_area"
                    assert summary.access_level in VALID_ACCESS_LEVELS, "Invalid access_level"

                    print("OK")
                    print(f"  Essence: {summary.essence[:100]}...")
                    print(f"  Concepts: {len(summary.key_concepts)}")
                    print(f"  Quotes: {len(summary.quotes)}")
                    print(f"  Topic area: {summary.topic_area}")
                    print(f"  Access level: {summary.access_level}")
                    print(f"  Insight: {summary.insight[:50]}..." if summary.insight else "  No insight")

                except Exception as e:
                    print(f"FAILED: {e}")
                    import traceback
                    traceback.print_exc()
                    return 1

        print("\n" + "=" * 40)
        print("All tests passed!")
        return 0

    sys.exit(asyncio.run(run_tests()))
