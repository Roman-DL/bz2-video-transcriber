# Model-specific configurations for pipeline stages
# Each model family can have different parameters based on its context window size

# Claude models available for selection in UI
# Only shown when ANTHROPIC_API_KEY is set
# Using aliases for auto-updates to latest versions
# Pricing: $ per 1M tokens (input/output)
claude_models:
  - id: "claude-sonnet-4-5"
    name: "Claude Sonnet 4.5"
    description: "Быстрая и умная ($3/$15 за 1M токенов)"
    pricing:
      input: 3.00
      output: 15.00
  - id: "claude-haiku-4-5"
    name: "Claude Haiku 4.5"
    description: "Самая быстрая ($1/$5 за 1M токенов)"
    pricing:
      input: 1.00
      output: 5.00
  - id: "claude-opus-4-5"
    name: "Claude Opus 4.5"
    description: "Максимальный интеллект ($15/$75 за 1M токенов)"
    pricing:
      input: 15.00
      output: 75.00

# Available Whisper models for transcription
# Only models listed here will be shown in the UI
whisper_models:
  - id: "Systran/faster-whisper-large-v3"
    name: "large-v3"
    description: "Высокое качество, медленнее"
  - id: "deepdml/faster-whisper-large-v3-turbo-ct2"
    name: "large-v3-turbo"
    description: "Быстрее, хорошее качество"


# =============================================================================
# Context Profiles
# =============================================================================
# Processing parameters based on context window size.
# DRY: define parameters once, reference by profile name.
# New models just need to specify their profile.

context_profiles:
  # Small context models (< 16K tokens)
  # For local models like gemma2:9b
  small:
    context_tokens: 8192
    cleaner:
      chunk_size: 3000
      chunk_overlap: 200
      small_text_threshold: 3500
    text_splitter:
      part_size: 6000
      overlap_size: 1500
      min_part_size: 2000
    longread:
      chunks_per_section: 4
      max_parallel_sections: 2
    summary:
      max_input_chars: 8000

  # Medium context models (16K - 64K tokens)
  # For models like qwen2.5:14b, llama3.2:8b
  medium:
    context_tokens: 32768
    cleaner:
      chunk_size: 8000
      chunk_overlap: 500
      small_text_threshold: 9000
    text_splitter:
      part_size: 15000
      overlap_size: 3000
      min_part_size: 5000
    longread:
      chunks_per_section: 6
      max_parallel_sections: 3
    summary:
      max_input_chars: 25000

  # Large context models (> 100K tokens)
  # For cloud models like Claude (200K context)
  large:
    context_tokens: 200000
    cleaner:
      # With 200K context, no chunking needed for most documents
      chunk_size: 100000
      chunk_overlap: 10000
      small_text_threshold: 150000
    text_splitter:
      part_size: 80000
      overlap_size: 10000
      min_part_size: 20000
    longread:
      chunks_per_section: 10
      max_parallel_sections: 4
    summary:
      max_input_chars: 100000


# =============================================================================
# AI Providers
# =============================================================================

providers:
  ollama:
    type: "local"
    default_profile: small
    base_url_env: "OLLAMA_URL"

  claude:
    type: "cloud"
    default_profile: large
    api_key_env: "ANTHROPIC_API_KEY"
    # Future: rate_limit, pricing, etc.


# =============================================================================
# Model Configurations
# =============================================================================
# Each model references a context_profile for its parameters.
# Override specific parameters if needed.

models:
  # Gemma 2 9B - production model for cleaning/chunking
  gemma2:
    provider: ollama
    context_profile: small
    context_tokens: 8192
    # All params inherited from 'small' profile

  # Qwen 2.5 14B - for longread/summary generation
  qwen2.5:
    provider: ollama
    context_profile: medium
    context_tokens: 32768
    # All params inherited from 'medium' profile

  # Qwen 3 - larger context variant
  qwen3:
    provider: ollama
    context_profile: medium
    context_tokens: 40960
    # Override specific params for larger context
    cleaner:
      chunk_size: 10000
      chunk_overlap: 600
      small_text_threshold: 11000
    text_splitter:
      part_size: 20000
      overlap_size: 4000
      min_part_size: 6000
    longread:
      chunks_per_section: 8
      max_parallel_sections: 4
    summary:
      max_input_chars: 40000

  # Claude Sonnet 4.5 - recommended for most tasks
  claude-sonnet-4-5:
    provider: claude
    context_profile: large
    context_tokens: 200000

  # Claude Haiku 4.5 - fastest, cheapest
  claude-haiku-4-5:
    provider: claude
    context_profile: large
    context_tokens: 200000

  # Claude Opus 4.5 - maximum intelligence
  claude-opus-4-5:
    provider: claude
    context_profile: large
    context_tokens: 200000


# =============================================================================
# Backward Compatibility: Legacy model keys
# =============================================================================
# Keep old format for existing code that uses model family names

  # Legacy: 'qwen2' key for backward compatibility
  qwen2:
    provider: ollama
    context_profile: medium
    context_tokens: 32768


# =============================================================================
# Default Fallback Values
# =============================================================================
# Used when model not found in configuration

defaults:
  provider: ollama
  context_profile: small
  cleaner:
    chunk_size: 3000
    chunk_overlap: 200
    small_text_threshold: 3500
  text_splitter:
    part_size: 6000
    overlap_size: 1500
    min_part_size: 2000
  longread:
    chunks_per_section: 4
    max_parallel_sections: 2
  summary:
    max_input_chars: 8000
